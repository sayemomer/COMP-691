{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 7: Lab Exercises for COMP499/691 Conversational AI**\n",
        "\n",
        "This lab focuses on pretraining and fine-tuning models for speech processing. The aim is to help you become more comfortable with these techniques. Throughout the lab, we will be exploring various strategies and methods to achieve this goal. We will also take this opportunity to compare the performance of a system fed by standard features with one fed by modern self-supervised features.\n",
        "\n",
        "## **Task Description**\n",
        "\n",
        "This time, our focus is on distinguishing between native and non-native English speakers. It's a straightforward task: we feed a speech signal into the model that determines whether or not the speaker is a native English speaker.\n",
        "\n",
        "\n",
        "Let's first download the dataset:"
      ],
      "metadata": {
        "id": "VhV7Sj9wOMTE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NT4eKurD05sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46b852f-faa2-4a70-eb8f-ca6e10dd9a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1jdS2Pnt1ptaBPQNFSrhAeQn5mwUt2JsQ\n",
            "From (redirected): https://drive.google.com/uc?id=1jdS2Pnt1ptaBPQNFSrhAeQn5mwUt2JsQ&confirm=t&uuid=b35da71a-627f-4c1a-8b10-b58a54ac6675\n",
            "To: /content/native_vs_nonnative.zip\n",
            "100% 216M/216M [00:02<00:00, 77.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown 1jdS2Pnt1ptaBPQNFSrhAeQn5mwUt2JsQ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warning:** If you cannot download the data due to an access denied error. Please, download the data from [here](https://drive.google.com/uc?id=1jdS2Pnt1ptaBPQNFSrhAeQn5mwUt2JsQ) and upload them manually on Google Colab (Files, upload)."
      ],
      "metadata": {
        "id": "yv8xkgTnRgwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now uncompress it:"
      ],
      "metadata": {
        "id": "DPvlmOYePgAI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JrPMmW8F55Aa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip native_vs_nonnative.zip -d data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have to install the needed libraries:"
      ],
      "metadata": {
        "id": "AoxE7nA6Pj-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/speechbrain.git\n",
        "%cd speechbrain\n",
        "!pip install -r requirements.txt\n",
        "!pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "id": "hu55IvWKaHz4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 1: Data Preparation**\n",
        "\n",
        "If you inspect the data stored in `/content/data/accent_data` you will see that the data are already organized into three chunks: *train*, *valid*, and *test*.\n",
        "Inside each folder, you can find a bunch of files like:\n",
        "\n",
        "- *native_1.wav*, *native_2.wav*, ..., *native_200.wav*\n",
        "- *non-native_1.wav*, *non-native_2.wav*,..., *non-native_200.wav*\n",
        "\n",
        "\n",
        "From the filename, you can already see if they refer to native or non-native speakers.\n",
        "\n",
        "\n",
        "**Write the code for preparing the JSON data-manifest files**.You have to create 3 JSON files:\n",
        "- 'train.json'\n",
        "- 'valid.json'\n",
        "- 'test.json'\n",
        "\n",
        "They should be formatted in the following way:\n",
        "\n",
        "\n",
        "**train.json**\n",
        "```\n",
        "{\n",
        "  \"native_82\": {\n",
        "    \"path\": \"data/accent_data/train/native_82.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"native\"\n",
        "  },\n",
        "  \"native_3\": {\n",
        "    \"path\": \"data/accent_data/train/native_3.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"native\"\n",
        "  },\n",
        "  \"non-native_44\": {\n",
        "    \"path\": \"data/accent_data/train/non-native_44.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"non-native\"\n",
        "  },\n",
        "....\n",
        "\n",
        "```\n",
        "\n",
        "**valid.json**\n",
        "```\n",
        "{\n",
        "  \"native_223\": {\n",
        "    \"path\": \"data/accent_data/valid/native_223.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"native\"\n",
        "  },\n",
        "  \"native_203\": {\n",
        "    \"path\": \"data/accent_data/valid/native_203.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"native\"\n",
        "  },\n",
        "  \"native_221\": {\n",
        "    \"path\": \"data/accent_data/valid/native_221.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"native\"\n",
        "  },\n",
        "  \"non-native_217\": {\n",
        "    \"path\": \"data/accent_data/valid/non-native_217.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"non-native\"\n",
        "  },\n",
        "....\n",
        "```\n",
        "\n",
        "**test.json**\n",
        "\n",
        "```\n",
        "{\n",
        "  \"non-native_227\": {\n",
        "    \"path\": \"data/accent_data/test/non-native_227.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"non-native\"\n",
        "  },\n",
        "  \"native_234\": {\n",
        "    \"path\": \"data/accent_data/test/native_234.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"native\"\n",
        "  },\n",
        "  \"non-native_232\": {\n",
        "    \"path\": \"data/accent_data/test/non-native_232.wav\",\n",
        "    \"length\": 8.0,\n",
        "    \"language\": \"non-native\"\n",
        "  },\n",
        "....\n",
        "```\n",
        "\n",
        "**Suggestions:**\n",
        "- Use the get_all_files in speechbrain.utils.data_utils to get a list of all the files with the wav extension.\n",
        "- You can get the number of samples of each wave with torchaudio.info. You have to compute the duration in seconds by diving it for the sampling frequency.\n"
      ],
      "metadata": {
        "id": "F-oauBTvPqja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import json\n",
        "from speechbrain.utils.data_utils import get_all_files\n",
        "import re\n",
        "import json\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "import os\n",
        "\n",
        "\n",
        "train_files = []\n",
        "test_files = []\n",
        "valid_files = []\n",
        "\n",
        "def loadFile(audiofiles,file):\n",
        "\n",
        "  for audiofile in audiofiles:\n",
        "    file.append(audiofile)\n",
        "\n",
        "\n",
        "train_location = get_all_files(os.path.join('/content/data/accent_data/train'), match_and=[\".wav\"])\n",
        "test_location = get_all_files(os.path.join('/content/data/accent_data/test'), match_and=[\".wav\"])\n",
        "valid_location = get_all_files(os.path.join('/content/data/accent_data/valid'), match_and=[\".wav\"])\n",
        "\n",
        "loadFile(train_location,train_files)\n",
        "loadFile(test_location,test_files)\n",
        "loadFile(valid_location,valid_files)\n",
        "\n",
        "# print(len(train_files))\n",
        "# print(len(test_files))\n",
        "# print(len(valid_files))\n",
        "\n",
        "\n",
        "\n",
        "def create_json(json_file, audiolist):\n",
        "  json_dict = {}\n",
        "  for audiofile in audiolist:\n",
        "\n",
        "    # Getting info\n",
        "    audioinfo = torchaudio.info(audiofile) # Your code here\n",
        "    signal = read_audio(audiofile)\n",
        "\n",
        "\n",
        "    # Compute the duration in seconds.\n",
        "    # This is the number of samples divided by the sampling frequency\n",
        "    duration = audioinfo.num_frames / audioinfo.sample_rate  # Your code here\n",
        "\n",
        "    path = audiofile\n",
        "\n",
        "    speaker = audiofile.split(\"/\")[-1].split(\"_\")[-2]\n",
        "\n",
        "    # Get a unique utterance id\n",
        "    uttid =  audiofile.split(\"/\")[-1].split(\".\")[-2]\n",
        "\n",
        "    # Create entry for this utterance\n",
        "    json_dict[uttid] = {\n",
        "            \"path\": path,\n",
        "            \"length\": duration,\n",
        "            \"language\": speaker,\n",
        "    }\n",
        "\n",
        "    # Writing the dictionary to the json file\n",
        "    with open(json_file, mode=\"w\", encoding=\"utf-8\") as json_f:\n",
        "      json.dump(json_dict, json_f, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "create_json('train.json', train_files)\n",
        "create_json('valid.json', test_files)\n",
        "create_json('test.json', valid_files)"
      ],
      "metadata": {
        "id": "bd4ehL01Zb-N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 2: Native vs Non-Native Classification with FBANKs and Xvectors**\n",
        "\n",
        "You have to implement the following model:\n",
        "\n"
      ],
      "metadata": {
        "id": "Ksu4kgeSTW9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![pictures-Page-1.drawio (7).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "i_ZdiUl2VJMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind that this system is intended for binary classification. Ideally, you should substitute the softmax with the sigmoid and the NNL loss with the binary cross-entropy. However, this modification will have minimal effect. Therefore, we suggest you to still use a softmax+NNL for simplicity.\n",
        "\n",
        "You can use the same optimization and architecture hyperparameters use in the previous lab (except for the number of classes):\n",
        "\n",
        "- n_mels: 40\n",
        "- sample_rate: 16000\n",
        "- number_of_epochs: 25\n",
        "- batch_size: 16\n",
        "- lr_start: 0.001\n",
        "- lr_final: 0.0001\n",
        "- emb_dim: 128\n",
        "\n",
        "Use the same TDNN architecture used so far.\n",
        "\n",
        "\n",
        "**Suggestion:** You can copy-and-paste the code from the previous labs solving the speaker identification problem. You only need to do the little modifications to the hyperparameter and train.py files needed to implement the system in the figure.\n"
      ],
      "metadata": {
        "id": "mbwnZ4bVVL5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write the code for the hyperparameters**:"
      ],
      "metadata": {
        "id": "7NSVEY8BTXDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file hparams_xvector_fbanks.yaml\n",
        "\n",
        "# Your code here\n",
        "\n",
        "# #################################\n",
        "# Basic training parameters for digit classification with Xvector\n",
        "#\n",
        "# Author:\n",
        "#  * Mirco Ravanelli 2021\n",
        "# #################################\n",
        "\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 1986\n",
        "__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "output_folder: !ref ./results/Xvector/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# Path where data manifest files are stored\n",
        "train_annotation: train.json\n",
        "valid_annotation: valid.json\n",
        "test_annotation: test.json\n",
        "\n",
        "# The train logger writes training statistics to a file, as well as stdout.\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "# Feature parameters\n",
        "n_mels: 40\n",
        "\n",
        "# Training Parameters\n",
        "sample_rate: 16000\n",
        "number_of_epochs: 25\n",
        "batch_size: 16\n",
        "lr_start: 0.001\n",
        "lr_final: 0.0001\n",
        "n_classes: 2 # In this case, this is binary classification\n",
        "emb_dim: 128 # dimensionality of the embeddings\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "\n",
        "# Feature extraction\n",
        "compute_features: !new:speechbrain.lobes.features.Fbank\n",
        "    n_mels: !ref <n_mels>\n",
        "    deltas: False\n",
        "    context: False\n",
        "    requires_grad: False\n",
        "    sample_rate: !ref <sample_rate>\n",
        "    n_fft:  400\n",
        "    param_change_factor:  1.0\n",
        "    param_rand_factor:  0.0\n",
        "    left_frames:  5\n",
        "    right_frames: 5\n",
        "    win_length: 25\n",
        "    hop_length: 10\n",
        "\n",
        "# Mean and std normalization of the input features\n",
        "mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n",
        "    norm_type: global\n",
        "\n",
        "# Embedding model: from variable size digits gets a fixed size embedding vector\n",
        "embedding_model: !new:speechbrain.lobes.models.Xvector.Xvector\n",
        "    in_channels: !ref <n_mels>\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    tdnn_blocks: 5\n",
        "    tdnn_channels: [64, 64, 64, 64, 128]\n",
        "    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n",
        "    tdnn_dilations: [1, 2, 3, 1, 1]\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "\n",
        "# Clasifier applied on top of the embeddings\n",
        "classifier: !new:speechbrain.lobes.models.Xvector.Classifier\n",
        "    input_shape: [null, null, !ref <emb_dim>]\n",
        "    activation: !name:torch.nn.LeakyReLU\n",
        "    lin_blocks: 1\n",
        "    lin_neurons: !ref <emb_dim>\n",
        "    out_neurons: !ref <n_classes>\n",
        "\n",
        "# The first object passed to the Brain class is this \"Epoch Counter\"\n",
        "# which is saved by the Checkpointer so that training can be resumed\n",
        "# if it gets interrupted at any point.\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "# Objects in \"modules\" dict will have their parameters moved to the correct\n",
        "# device, as well as having train()/eval() called on them by the Brain class.\n",
        "modules:\n",
        "    compute_features: !ref <compute_features>\n",
        "    mean_var_norm: !ref <mean_var_norm>\n",
        "    embedding_model: !ref <embedding_model>\n",
        "    classifier: !ref <classifier>\n",
        "\n",
        "# This optimizer will be constructed by the Brain class after all parameters\n",
        "# are moved to the correct device. Then it will be added to the checkpointer.\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_start>\n",
        "\n",
        "# This function manages learning rate annealing over the epochs.\n",
        "# We here use the simple lr annealing method that linearly decreases\n",
        "# the lr from the initial value to the final one.\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
        "    initial_value: !ref <lr_start>\n",
        "    final_value: !ref <lr_final>\n",
        "    epoch_count: !ref <number_of_epochs>\n",
        "\n",
        "# This object is used for saving the state of training both so that it\n",
        "# can be resumed if it gets interrupted, and also so that the best checkpoint\n",
        "# can be later loaded for evaluation or inference.\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        embedding_model: !ref <embedding_model>\n",
        "        classifier: !ref <classifier>\n",
        "        normalizer: !ref <mean_var_norm>\n",
        "        counter: !ref <epoch_counter>"
      ],
      "metadata": {
        "id": "KCw2hcG-6k_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b4707e-320e-49a8-bf9a-750f7f6a4f72"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hparams_xvector_fbanks.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write the code for the training script**:"
      ],
      "metadata": {
        "id": "8Wyg6u9qWE2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file train_fbanks.py\n",
        "# Your code here\n",
        "\n",
        "import torchaudio.functional as F\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"Recipe for training a digit classification system.\"\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "\n",
        "# Brain class for speech enhancement training\n",
        "class DigitBrain(sb.Brain):\n",
        "    \"\"\"Class that manages the training loop. See speechbrain.core.Brain.\"\"\"\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Runs all the computations that transforms the input into the\n",
        "        output probabilities over the N classes.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        predictions : Tensor\n",
        "            Tensor that contains the posterior probabilities over the N classes.\n",
        "        \"\"\"\n",
        "        # Your code here. Aim for 7-8 lines\n",
        "\n",
        "        # We first move the batch to the appropriate device.\n",
        "        batch = batch.to(self.device)\n",
        "\n",
        "\n",
        "        # Compute features, embeddings, and predictions\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        # Feature extraction and normalization\n",
        "        feats = self.modules.compute_features(wavs)\n",
        "        feats = self.modules.mean_var_norm(feats, lens)\n",
        "\n",
        "        embeddings = self.modules.embedding_model(feats, lens)\n",
        "        predictions = self.modules.classifier(embeddings)\n",
        "\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss given the predicted and targeted outputs.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        predictions : tensor\n",
        "            The output tensor from `compute_forward`.\n",
        "        batch : PaddedBatch\n",
        "            This batch object contains all the relevant tensors for computation.\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        Returns\n",
        "        -------\n",
        "        loss : torch.Tensor\n",
        "            A one-element tensor used for backpropagating the gradient.\n",
        "        \"\"\"\n",
        "        # Your code here. Aim for 7-8 lines\n",
        "\n",
        "        _, lens = batch.sig\n",
        "\n",
        "        spkid, _ = batch.digit_encoded\n",
        "\n",
        "        # Compute the cost function\n",
        "        loss = sb.nnet.losses.nll_loss(predictions, spkid, lens)\n",
        "\n",
        "        # Append this batch of losses to the loss metric for easy\n",
        "        self.loss_metric.append(\n",
        "            batch.id, predictions, spkid, lens, reduction=\"batch\"\n",
        "        )\n",
        "\n",
        "        # Compute classification error at test time\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(batch.id, predictions, spkid, lens)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of each epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set up statistics trackers for this stage\n",
        "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
        "            metric=sb.nnet.losses.nll_loss\n",
        "        )\n",
        "\n",
        "        # Set up evaluation-only statistics trackers\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
        "        stage_loss : float\n",
        "            The average loss for all of the data processed in this stage.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Store the train loss until the validation stage.\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_loss = stage_loss\n",
        "\n",
        "        # Summarize the statistics from the stage for record-keeping.\n",
        "        else:\n",
        "            stats = {\n",
        "                \"loss\": stage_loss,\n",
        "                \"error\": self.error_metrics.summarize(\"average\"),\n",
        "            }\n",
        "\n",
        "        # At the end of validation...\n",
        "        if stage == sb.Stage.VALID:\n",
        "\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(epoch)\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            # The train_logger writes a summary to stdout and to the logfile.\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch\": epoch, \"lr\": old_lr},\n",
        "                train_stats={\"loss\": self.train_loss},\n",
        "                valid_stats=stats,\n",
        "            )\n",
        "\n",
        "            # Save the current checkpoint and delete previous checkpoints,\n",
        "            self.checkpointer.save_and_keep_only(meta=stats, min_keys=[\"error\"])\n",
        "\n",
        "        # We also write statistics about test data to stdout and to the logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stats,\n",
        "            )\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\n",
        "    We expect `prepare_mini_librispeech` to have been called before this,\n",
        "    so that the `train.json`, `valid.json`,  and `valid.json` manifest files\n",
        "    are available.\n",
        "    Arguments\n",
        "    ---------\n",
        "    hparams : dict\n",
        "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
        "        all the hyperparameters needed for dataset construction and loading.\n",
        "    Returns\n",
        "    -------\n",
        "    datasets : dict\n",
        "        Contains two keys, \"train\" and \"valid\" that correspond\n",
        "        to the appropriate DynamicItemDataset object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialization of the label encoder. The label encoder assigns to each\n",
        "    # of the observed label a unique index (e.g, 'digit0': 0, 'digit1': 1, ..)\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # Define audio pipeline\n",
        "    @sb.utils.data_pipeline.takes(\"path\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav):\n",
        "        \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
        "        This is done on the CPU in the `collate_fn`.\"\"\"\n",
        "        sig, fs = torchaudio.load(wav)\n",
        "\n",
        "        # Resampling\n",
        "        # sig = # Your code here. Aim for 1-2 lines\n",
        "        sig = sb.dataio.dataio.read_audio(wav)\n",
        "\n",
        "        # Get original sample rate\n",
        "        # original_fs = torchaudio.info(wav).sample_rate  # Fetch sample rate from file\n",
        "\n",
        "        # # Resample only if needed\n",
        "        # if original_fs != 16000:\n",
        "        #     sig = F.resample(sig, orig_freq=original_fs, new_freq=16000)\n",
        "\n",
        "        sig =  sig.squeeze()\n",
        "\n",
        "\n",
        "        return sig\n",
        "\n",
        "    # Define label pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"language\")\n",
        "    @sb.utils.data_pipeline.provides(\"language\", \"digit_encoded\")\n",
        "    def label_pipeline(digit):\n",
        "        \"\"\"Defines the pipeline to process the digit labels.\n",
        "        Note that we have to assign a different integer to each class\n",
        "        through the label encoder.\n",
        "        \"\"\"\n",
        "        yield digit\n",
        "        digit_encoded = label_encoder.encode_label_torch(digit)\n",
        "        yield digit_encoded\n",
        "\n",
        "    # Define datasets. We also connect the dataset with the data processing\n",
        "    # functions defined above.\n",
        "    datasets = {}\n",
        "    data_info = {\n",
        "        \"train\": hparams[\"train_annotation\"],\n",
        "        \"valid\": hparams[\"valid_annotation\"],\n",
        "        \"test\": hparams[\"test_annotation\"],\n",
        "    }\n",
        "    hparams[\"dataloader_options\"][\"shuffle\"] = True\n",
        "    for dataset in data_info:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=data_info[dataset],\n",
        "            dynamic_items=[audio_pipeline, label_pipeline],\n",
        "            output_keys=[\"id\", \"sig\", \"digit_encoded\"],\n",
        "        )\n",
        "\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    # Please, take a look into the lab_enc_file to see the label to index\n",
        "    # mapping.\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[datasets[\"train\"]],\n",
        "        output_key=\"language\",\n",
        "    )\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Recipe begins!\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Reading command line arguments.\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides.\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin,  overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Create dataset objects \"train\", \"valid\", and \"test\".\n",
        "    datasets = dataio_prep(hparams)\n",
        "\n",
        "    # Initialize the Brain object to prepare for mask training.\n",
        "    digit_brain = DigitBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # The `fit()` method iterates the training loop, calling the methods\n",
        "    # necessary to update the parameters of the model. Since all objects\n",
        "    # with changing state are managed by the Checkpointer, training can be\n",
        "    # stopped at any point, and will be resumed on next call.\n",
        "    digit_brain.fit(\n",
        "        epoch_counter=digit_brain.hparams.epoch_counter,\n",
        "        train_set=datasets[\"train\"],\n",
        "        valid_set=datasets[\"valid\"],\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = digit_brain.evaluate(\n",
        "        test_set=datasets[\"test\"],\n",
        "        min_key=\"error\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seDk_Gzu6wUG",
        "outputId": "9adc40ee-6ef2-4ac2-c0c3-b018e56258c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_fbanks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the code below** to train the model"
      ],
      "metadata": {
        "id": "hi9CivYeWJ5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the output folder to start training from scratch\n",
        "# (and not from a previous checkpoint).\n",
        "!rm -rf ./results\n",
        "\n",
        "# Run Training\n",
        "!python train_fbanks.py hparams_xvector_fbanks.yaml  --device='cuda:0' --number_of_epochs=25"
      ],
      "metadata": {
        "id": "hBGeIwI962SO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8e44a2-3120-42f7-f4ae-e82fa5df9e22"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
            "speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
            "speechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: ./results/Xvector/1986\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "/usr/local/lib/python3.11/dist-packages/speechbrain/core.py:798: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=gradscaler_enabled)\n",
            "speechbrain.core - DigitBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 101.0k\n",
            "* Total Number of Parameters: 101.0k\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 25/25 [00:04<00:00,  5.84it/s, train_loss=0.687]\n",
            "100% 4/4 [00:00<00:00, 11.83it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.001 to 0.00096\n",
            "speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-03 - train loss: 6.87e-01 - valid loss: 6.53e-01, valid error: 4.80e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-32+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 25/25 [00:03<00:00,  7.90it/s, train_loss=0.584]\n",
            "100% 4/4 [00:00<00:00,  8.59it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00096 to 0.00093\n",
            "speechbrain.utils.train_logger - Epoch: 2, lr: 9.63e-04 - train loss: 5.84e-01 - valid loss: 7.12e-01, valid error: 4.80e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-36+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-32+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 25/25 [00:02<00:00,  8.78it/s, train_loss=0.521]\n",
            "100% 4/4 [00:00<00:00, 11.41it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00093 to 0.00089\n",
            "speechbrain.utils.train_logger - Epoch: 3, lr: 9.25e-04 - train loss: 5.21e-01 - valid loss: 5.86e-01, valid error: 3.80e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-39+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-36+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 25/25 [00:02<00:00,  9.18it/s, train_loss=0.434]\n",
            "100% 4/4 [00:00<00:00, 12.85it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00089 to 0.00085\n",
            "speechbrain.utils.train_logger - Epoch: 4, lr: 8.88e-04 - train loss: 4.34e-01 - valid loss: 6.81e-01, valid error: 3.80e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-42+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-39+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 25/25 [00:02<00:00,  9.21it/s, train_loss=0.432]\n",
            "100% 4/4 [00:00<00:00, 12.58it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00085 to 0.00081\n",
            "speechbrain.utils.train_logger - Epoch: 5, lr: 8.50e-04 - train loss: 4.32e-01 - valid loss: 7.25e-01, valid error: 3.80e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-45+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100% 25/25 [00:03<00:00,  7.34it/s, train_loss=0.406]\n",
            "100% 4/4 [00:00<00:00, 12.60it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00081 to 0.00078\n",
            "speechbrain.utils.train_logger - Epoch: 6, lr: 8.13e-04 - train loss: 4.06e-01 - valid loss: 7.14e-01, valid error: 3.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-49+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-45+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100% 25/25 [00:02<00:00,  9.12it/s, train_loss=0.333]\n",
            "100% 4/4 [00:00<00:00, 13.24it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00078 to 0.00074\n",
            "speechbrain.utils.train_logger - Epoch: 7, lr: 7.75e-04 - train loss: 3.33e-01 - valid loss: 6.81e-01, valid error: 3.40e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100% 25/25 [00:02<00:00,  9.39it/s, train_loss=0.294]\n",
            "100% 4/4 [00:00<00:00, 13.13it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00074 to 0.0007\n",
            "speechbrain.utils.train_logger - Epoch: 8, lr: 7.38e-04 - train loss: 2.94e-01 - valid loss: 4.82e-01, valid error: 3.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-55+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-49+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-52+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100% 25/25 [00:02<00:00,  9.21it/s, train_loss=0.227]\n",
            "100% 4/4 [00:00<00:00, 13.14it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0007 to 0.00066\n",
            "speechbrain.utils.train_logger - Epoch: 9, lr: 7.00e-04 - train loss: 2.27e-01 - valid loss: 7.52e-01, valid error: 3.80e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-58+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100% 25/25 [00:03<00:00,  7.25it/s, train_loss=0.277]\n",
            "100% 4/4 [00:00<00:00, 13.46it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00066 to 0.00063\n",
            "speechbrain.utils.train_logger - Epoch: 10, lr: 6.63e-04 - train loss: 2.77e-01 - valid loss: 5.83e-01, valid error: 2.40e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-02+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-55+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-35-58+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 11\n",
            "100% 25/25 [00:02<00:00,  9.25it/s, train_loss=0.207]\n",
            "100% 4/4 [00:00<00:00, 12.98it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00063 to 0.00059\n",
            "speechbrain.utils.train_logger - Epoch: 11, lr: 6.25e-04 - train loss: 2.07e-01 - valid loss: 5.80e-01, valid error: 2.80e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 12\n",
            "100% 25/25 [00:02<00:00,  9.27it/s, train_loss=0.209]\n",
            "100% 4/4 [00:00<00:00, 12.86it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00059 to 0.00055\n",
            "speechbrain.utils.train_logger - Epoch: 12, lr: 5.88e-04 - train loss: 2.09e-01 - valid loss: 5.74e-01, valid error: 3.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-08+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 13\n",
            "100% 25/25 [00:02<00:00,  8.62it/s, train_loss=0.14]\n",
            "100% 4/4 [00:00<00:00,  8.94it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00055 to 0.00051\n",
            "speechbrain.utils.train_logger - Epoch: 13, lr: 5.50e-04 - train loss: 1.40e-01 - valid loss: 4.91e-01, valid error: 2.40e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-11+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-08+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-02+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 14\n",
            "100% 25/25 [00:03<00:00,  8.04it/s, train_loss=0.121]\n",
            "100% 4/4 [00:00<00:00, 13.12it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00051 to 0.00048\n",
            "speechbrain.utils.train_logger - Epoch: 14, lr: 5.13e-04 - train loss: 1.21e-01 - valid loss: 5.85e-01, valid error: 2.80e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-15+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 15\n",
            "100% 25/25 [00:02<00:00,  9.21it/s, train_loss=0.122]\n",
            "100% 4/4 [00:00<00:00, 13.45it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00048 to 0.00044\n",
            "speechbrain.utils.train_logger - Epoch: 15, lr: 4.75e-04 - train loss: 1.22e-01 - valid loss: 5.20e-01, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-18+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-15+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-11+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 16\n",
            "100% 25/25 [00:02<00:00,  9.14it/s, train_loss=0.0984]\n",
            "100% 4/4 [00:00<00:00, 13.01it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00044 to 0.0004\n",
            "speechbrain.utils.train_logger - Epoch: 16, lr: 4.38e-04 - train loss: 9.84e-02 - valid loss: 4.98e-01, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-21+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-18+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 17\n",
            "100% 25/25 [00:03<00:00,  7.70it/s, train_loss=0.0624]\n",
            "100% 4/4 [00:00<00:00,  8.85it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0004 to 0.00036\n",
            "speechbrain.utils.train_logger - Epoch: 17, lr: 4.00e-04 - train loss: 6.24e-02 - valid loss: 4.75e-01, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-25+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-21+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 18\n",
            "100% 25/25 [00:02<00:00,  9.10it/s, train_loss=0.0893]\n",
            "100% 4/4 [00:00<00:00, 13.72it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00036 to 0.00033\n",
            "speechbrain.utils.train_logger - Epoch: 18, lr: 3.63e-04 - train loss: 8.93e-02 - valid loss: 6.35e-01, valid error: 3.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-28+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 19\n",
            "100% 25/25 [00:02<00:00,  9.13it/s, train_loss=0.0626]\n",
            "100% 4/4 [00:00<00:00, 13.39it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00033 to 0.00029\n",
            "speechbrain.utils.train_logger - Epoch: 19, lr: 3.25e-04 - train loss: 6.26e-02 - valid loss: 5.43e-01, valid error: 2.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-31+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-28+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 20\n",
            "100% 25/25 [00:02<00:00,  9.26it/s, train_loss=0.0518]\n",
            "100% 4/4 [00:00<00:00, 13.07it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00029 to 0.00025\n",
            "speechbrain.utils.train_logger - Epoch: 20, lr: 2.88e-04 - train loss: 5.18e-02 - valid loss: 4.70e-01, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-34+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-25+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-31+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 21\n",
            "100% 25/25 [00:03<00:00,  7.38it/s, train_loss=0.109]\n",
            "100% 4/4 [00:00<00:00, 13.06it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00025 to 0.00021\n",
            "speechbrain.utils.train_logger - Epoch: 21, lr: 2.50e-04 - train loss: 1.09e-01 - valid loss: 5.49e-01, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-38+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-34+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 22\n",
            "100% 25/25 [00:02<00:00,  9.18it/s, train_loss=0.0449]\n",
            "100% 4/4 [00:00<00:00, 13.04it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00021 to 0.00017\n",
            "speechbrain.utils.train_logger - Epoch: 22, lr: 2.13e-04 - train loss: 4.49e-02 - valid loss: 5.30e-01, valid error: 2.40e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-41+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 23\n",
            "100% 25/25 [00:02<00:00,  9.25it/s, train_loss=0.0287]\n",
            "100% 4/4 [00:00<00:00, 12.83it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00017 to 0.00014\n",
            "speechbrain.utils.train_logger - Epoch: 23, lr: 1.75e-04 - train loss: 2.87e-02 - valid loss: 5.25e-01, valid error: 2.40e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-44+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-41+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 24\n",
            "100% 25/25 [00:02<00:00,  9.38it/s, train_loss=0.029]\n",
            "100% 4/4 [00:00<00:00, 12.50it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.00014 to 0.0001\n",
            "speechbrain.utils.train_logger - Epoch: 24, lr: 1.37e-04 - train loss: 2.90e-02 - valid loss: 5.40e-01, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-47+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-38+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-44+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 25\n",
            "100% 25/25 [00:03<00:00,  7.43it/s, train_loss=0.0407]\n",
            "100% 4/4 [00:00<00:00, 13.19it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 25, lr: 1.00e-04 - train loss: 4.07e-02 - valid loss: 5.20e-01, valid error: 2.00e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-51+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/Xvector/1986/save/CKPT+2025-03-17+02-36-47+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/Xvector/1986/save/CKPT+2025-03-17+02-36-51+00\n",
            "100% 4/4 [00:00<00:00, 12.85it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 25 - test loss: 4.19e-01, test error: 1.80e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything goes well, you should obtain something similar to the following training logs:\n",
        "\n",
        "\n",
        "```\n",
        "Epoch: 1, lr: 1.00e-03 - train loss: 6.77e-01 - valid loss: 6.02e-01, valid error: 4.40e-01\n",
        "Epoch: 2, lr: 9.63e-04 - train loss: 5.72e-01 - valid loss: 6.00e-01, valid error: 3.60e-01\n",
        "Epoch: 3, lr: 9.25e-04 - train loss: 4.69e-01 - valid loss: 8.11e-01, valid error: 4.00e-01\n",
        "Epoch: 4, lr: 8.88e-04 - train loss: 4.03e-01 - valid loss: 6.06e-01, valid error: 4.00e-01\n",
        "Epoch: 5, lr: 8.50e-04 - train loss: 3.26e-01 - valid loss: 8.52e-01, valid error: 4.00e-01\n",
        "Epoch: 6, lr: 8.13e-04 - train loss: 2.59e-01 - valid loss: 6.26e-01, valid error: 4.00e-01\n",
        "Epoch: 7, lr: 7.75e-04 - train loss: 1.79e-01 - valid loss: 8.85e-01, valid error: 3.80e-01\n",
        "Epoch: 8, lr: 7.38e-04 - train loss: 1.45e-01 - valid loss: 1.28, valid error: 3.80e-01\n",
        "Epoch: 9, lr: 7.00e-04 - train loss: 1.72e-01 - valid loss: 1.36, valid error: 3.40e-01\n",
        "Epoch: 10, lr: 6.63e-04 - train loss: 1.15e-01 - valid loss: 1.85, valid error: 3.20e-01\n",
        "Epoch: 11, lr: 6.25e-04 - train loss: 1.18e-01 - valid loss: 1.05, valid error: 3.60e-01\n",
        "Epoch: 12, lr: 5.88e-04 - train loss: 9.65e-02 - valid loss: 1.05, valid error: 3.60e-01\n",
        "Epoch: 13, lr: 5.50e-04 - train loss: 1.06e-01 - valid loss: 5.81e-01, valid error: 2.40e-01\n",
        "Epoch: 14, lr: 5.13e-04 - train loss: 5.25e-02 - valid loss: 6.20e-01, valid error: 2.80e-01\n",
        "Epoch: 15, lr: 4.75e-04 - train loss: 4.45e-02 - valid loss: 4.11e-01, valid error: 2.00e-01\n",
        "Epoch: 16, lr: 4.38e-04 - train loss: 3.06e-02 - valid loss: 5.14e-01, valid error: 2.60e-01\n",
        "Epoch: 17, lr: 4.00e-04 - train loss: 1.50e-02 - valid loss: 6.28e-01, valid error: 2.60e-01\n",
        "Epoch: 18, lr: 3.63e-04 - train loss: 7.78e-03 - valid loss: 5.71e-01, valid error: 2.40e-01\n",
        "Epoch: 19, lr: 3.25e-04 - train loss: 5.88e-03 - valid loss: 5.69e-01, valid error: 2.40e-01\n",
        "Epoch: 20, lr: 2.88e-04 - train loss: 5.10e-03 - valid loss: 5.81e-01, valid error: 2.40e-01\n",
        "Epoch: 21, lr: 2.50e-04 - train loss: 4.60e-03 - valid loss: 5.89e-01, valid error: 2.40e-01\n",
        "Epoch: 22, lr: 2.13e-04 - train loss: 4.25e-03 - valid loss: 5.94e-01, valid error: 2.40e-01\n",
        "Epoch: 23, lr: 1.75e-04 - train loss: 4.00e-03 - valid loss: 5.98e-01, valid error: 2.40e-01\n",
        "Epoch: 24, lr: 1.37e-04 - train loss: 3.81e-03 - valid loss: 6.01e-01, valid error: 2.40e-01\n",
        "Epoch: 25, lr: 1.00e-04 - train loss: 3.67e-03 - valid loss: 6.03e-01, valid error: 2.40e-01\n",
        "Epoch loaded: 25 - test loss: 5.67e-01, test error: 3.20e-01\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "The expected test error rate is about 32%. Rather big variations (e.g, 38% are possible due to the random initialization). The network quickly overfits, leading to poor performance. This is because we are working with a small dataset and starting the training process from scratch."
      ],
      "metadata": {
        "id": "msdhOduOWV2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3: Native vs Non-Native Classification with Wav2Vec2**\n",
        "\n",
        "In this exercise, you will implement a system based on a self-supervised encoder (i.e., wav2vec2.0, Hubert, and WavLM) followed by a linear classifier.\n",
        "\n",
        "The system is the following:"
      ],
      "metadata": {
        "id": "awzZagi-XpUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![pictures-Page-1.drawio (9).png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "r4GhkuArZwqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also here, in theory, you should replace the softmax with the sigmoid and the NNL loss with the binary cross-entropy. However, this has basically no impact on the performance and we suggest you to still use a softmax+NNL for simplicity.\n",
        "\n",
        "We will start by implementing a system based on **wav2vec2**. We have to pretrain the encoder and fine-tuning it jointly with the linear classifier. We train the model for 5 traning epochs only. For the lab assignment, you need to report the result with Wav2vec2. But, we encourage you to try Hubert and Wavlm and compare the performance.\n",
        "\n",
        "\n",
        "**Suggestion:** This system is very similar to the one see in the tutorial of this week. You can copy-and-paste the code from the tutorial solving the speaker emotion recognition problem. You only need to do the little modifications to the hyperparameter and train.py files needed to implement the system in the figure. Also, use the same hyperparameters (expect for the number of classes)."
      ],
      "metadata": {
        "id": "yRzr7S5DZ77T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X0LL3Qbb0ClL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write the code for the hyperparameters here:**"
      ],
      "metadata": {
        "id": "-H8VX7ApbNt0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Vj8O4cLCGMRX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec93a88-d586-4433-bfd6-4c20abf1f34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hparams_wav2vec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_wav2vec.yaml\n",
        "\n",
        "# Your code here\n",
        "\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 1993\n",
        "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# Dataset will be downloaded to the `data_original`\n",
        "data_folder: !PLACEHOLDER  # e.g., /path/to/IEMOCAP_full_release\n",
        "output_folder: !ref /content/results/train_with_wav2vec2/<seed>\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# URL for the ssl model, you can change to benchmark diffrenet models\n",
        "# Important: we use wav2vec2 base and not the fine-tuned one with ASR task\n",
        "# This allow you to have ~4% improvment\n",
        "sslmodel_hub: facebook/wav2vec2-base\n",
        "sslmodel_folder: !ref <save_folder>/ssl_checkpoint\n",
        "\n",
        "# Path where data manifest files will be stored\n",
        "train_annotation: !ref <data_folder>/train.json\n",
        "valid_annotation: !ref <data_folder>/valid.json\n",
        "test_annotation: !ref <data_folder>/test.json\n",
        "\n",
        "# The train logger writes training statistics to a file, as well as stdout.\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "number_of_epochs: 30\n",
        "batch_size: 4\n",
        "lr: 0.0001\n",
        "lr_ssl: 0.00001\n",
        "\n",
        "#freeze all ssl\n",
        "freeze_ssl: False\n",
        "#set to true to freeze the CONV part of the ssl model\n",
        "# We see an improvement of 2% with freezing CNNs\n",
        "freeze_ssl_conv: True\n",
        "\n",
        "####################### Model Parameters #######################################\n",
        "encoder_dim: 768\n",
        "\n",
        "# Number of emotions\n",
        "out_n_neurons: 2 # (native , Non-native)\n",
        "\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    shuffle: True\n",
        "    num_workers: 2  # 2 on linux but 0 works on windows\n",
        "    drop_last: False\n",
        "\n",
        "# ssl encoder\n",
        "ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <sslmodel_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_ssl>\n",
        "    freeze_feature_extractor: !ref <freeze_ssl_conv>\n",
        "    save_path: !ref <sslmodel_folder>\n",
        "\n",
        "avg_pool: !new:speechbrain.nnet.pooling.StatisticsPooling\n",
        "    return_std: False\n",
        "\n",
        "output_mlp: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <encoder_dim>\n",
        "    n_neurons: !ref <out_n_neurons>\n",
        "    bias: False\n",
        "\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "modules:\n",
        "    ssl_model: !ref <ssl_model>\n",
        "    output_mlp: !ref <output_mlp>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <output_mlp>]\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "compute_cost: !name:speechbrain.nnet.losses.nll_loss\n",
        "\n",
        "error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n",
        "    metric: !name:speechbrain.nnet.losses.classification_error\n",
        "        reduction: batch\n",
        "\n",
        "opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "\n",
        "ssl_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_ssl>\n",
        "\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_ssl: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_ssl>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        model: !ref <model>\n",
        "        ssl_model: !ref <ssl_model>\n",
        "        lr_annealing_output: !ref <lr_annealing>\n",
        "        lr_annealing_ssl: !ref <lr_annealing_ssl>\n",
        "        counter: !ref <epoch_counter>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYcKmhPmlALt"
      },
      "source": [
        "**Write the code for the training script here:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TTjZ1nIkHBW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60072658-c054-4fb3-e8fe-ad637cb8c0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "%%file train.py\n",
        "\n",
        "# Your code here\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Recipe for training an emotion recognition system from speech data only using IEMOCAP.\n",
        "The system classifies 4 emotions ( anger, happiness, sadness, neutrality) with a SSL model.\n",
        "\n",
        "To run this recipe, do the following:\n",
        "> python train.py hparams/train.yaml --data_folder /path/to/IEMOCAP_full_release\n",
        "\n",
        "Authors\n",
        " * Yingzhi WANG 2021\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import speechbrain as sb\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "\n",
        "class EmoIdBrain(sb.Brain):\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Computation pipeline based on a encoder + emotion classifier.\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, lens = batch.sig\n",
        "\n",
        "        outputs = self.modules.ssl_model(wavs, lens)\n",
        "\n",
        "        # last dim will be used for AdaptativeAVG pool\n",
        "        outputs = self.hparams.avg_pool(outputs, lens)\n",
        "        outputs = outputs.view(outputs.shape[0], -1)\n",
        "\n",
        "        outputs = self.modules.output_mlp(outputs)\n",
        "        outputs = self.hparams.log_softmax(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss using speaker-id as label.\n",
        "        \"\"\"\n",
        "        emoid, _ = batch.emo_encoded\n",
        "\n",
        "        \"\"\"to meet the input form of nll loss\"\"\"\n",
        "        emoid = emoid.squeeze(1)\n",
        "        loss = self.hparams.compute_cost(predictions, emoid)\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics.append(batch.id, predictions, emoid)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch=None):\n",
        "        \"\"\"Gets called at the beginning of each epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set up statistics trackers for this stage\n",
        "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
        "            metric=sb.nnet.losses.nll_loss\n",
        "        )\n",
        "\n",
        "        # Set up evaluation-only statistics trackers\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.error_metrics = self.hparams.error_stats()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
        "        \"\"\"Gets called at the end of an epoch.\n",
        "        Arguments\n",
        "        ---------\n",
        "        stage : sb.Stage\n",
        "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
        "        stage_loss : float\n",
        "            The average loss for all of the data processed in this stage.\n",
        "        epoch : int\n",
        "            The currently-starting epoch. This is passed\n",
        "            `None` during the test stage.\n",
        "        \"\"\"\n",
        "\n",
        "        # Store the train loss until the validation stage.\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_loss = stage_loss\n",
        "\n",
        "        # Summarize the statistics from the stage for record-keeping.\n",
        "        else:\n",
        "            stats = {\n",
        "                \"loss\": stage_loss,\n",
        "                \"error_rate\": self.error_metrics.summarize(\"average\"),\n",
        "            }\n",
        "\n",
        "        # At the end of validation...\n",
        "        if stage == sb.Stage.VALID:\n",
        "\n",
        "            old_lr, new_lr = self.hparams.lr_annealing(stats[\"error_rate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
        "\n",
        "            (\n",
        "                old_lr_ssl,\n",
        "                new_lr_ssl,\n",
        "            ) = self.hparams.lr_annealing_ssl(stats[\"error_rate\"])\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.ssl_optimizer, new_lr_ssl\n",
        "            )\n",
        "\n",
        "            # The train_logger writes a summary to stdout and to the logfile.\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch\": epoch, \"lr\": old_lr, \"ssl_lr\": old_lr_ssl},\n",
        "                train_stats={\"loss\": self.train_loss},\n",
        "                valid_stats=stats,\n",
        "            )\n",
        "\n",
        "            # Save the current checkpoint and delete previous checkpoints,\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta=stats, min_keys=[\"error_rate\"]\n",
        "            )\n",
        "\n",
        "        # We also write statistics about test data to stdout and to logfile.\n",
        "        if stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stats,\n",
        "            )\n",
        "\n",
        "    def init_optimizers(self):\n",
        "        \"Initializes the ssl optimizer and model optimizer\"\n",
        "        self.ssl_optimizer = self.hparams.ssl_opt_class(\n",
        "            self.modules.ssl_model.parameters()\n",
        "        )\n",
        "        self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
        "\n",
        "        if self.checkpointer is not None:\n",
        "            self.checkpointer.add_recoverable(\n",
        "                \"ssl_opt\", self.ssl_optimizer\n",
        "            )\n",
        "            self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
        "\n",
        "        self.optimizers_dict = {\n",
        "            \"model_optimizer\": self.optimizer,\n",
        "            \"ssl_optimizer\": self.ssl_optimizer,\n",
        "        }\n",
        "\n",
        "\n",
        "def dataio_prep(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined\n",
        "    functions. We expect `prepare_mini_librispeech` to have been called before\n",
        "    this, so that the `train.json`, `valid.json`,  and `valid.json` manifest\n",
        "    files are available.\n",
        "    Arguments\n",
        "    ---------\n",
        "    hparams : dict\n",
        "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
        "        all the hyperparameters needed for dataset construction and loading.\n",
        "    Returns\n",
        "    -------\n",
        "    datasets : dict\n",
        "        Contains two keys, \"train\" and \"valid\" that correspond\n",
        "        to the appropriate DynamicItemDataset object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define audio pipeline\n",
        "    @sb.utils.data_pipeline.takes(\"path\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav):\n",
        "        \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
        "        This is done on the CPU in the `collate_fn`.\"\"\"\n",
        "        sig = sb.dataio.dataio.read_audio(wav)\n",
        "        return sig\n",
        "\n",
        "    # Initialization of the label encoder. The label encoder assignes to each\n",
        "    # of the observed label a unique index (e.g, 'spk01': 0, 'spk02': 1, ..)\n",
        "    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
        "\n",
        "    # Define label pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"language\")\n",
        "    @sb.utils.data_pipeline.provides(\"language\", \"emo_encoded\")\n",
        "    def label_pipeline(emo):\n",
        "        yield emo\n",
        "        emo_encoded = label_encoder.encode_label_torch(emo)\n",
        "        yield emo_encoded\n",
        "\n",
        "    # Define datasets. We also connect the dataset with the data processing\n",
        "    # functions defined above.\n",
        "    datasets = {}\n",
        "    data_info = {\n",
        "        \"train\": hparams[\"train_annotation\"],\n",
        "        \"valid\": hparams[\"valid_annotation\"],\n",
        "        \"test\": hparams[\"test_annotation\"],\n",
        "    }\n",
        "    for dataset in data_info:\n",
        "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
        "            json_path=data_info[dataset],\n",
        "            replacements={\"data_root\": hparams[\"data_folder\"]},\n",
        "            dynamic_items=[audio_pipeline, label_pipeline],\n",
        "            output_keys=[\"id\", \"sig\", \"emo_encoded\"],\n",
        "        )\n",
        "    # Load or compute the label encoder (with multi-GPU DDP support)\n",
        "    # Please, take a look into the lab_enc_file to see the label to index\n",
        "    # mappinng.\n",
        "\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[datasets[\"train\"]],\n",
        "        output_key=\"language\",\n",
        "    )\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "# RECIPE BEGINS!\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Reading command line arguments.\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "\n",
        "    # Initialize ddp (useful only for multi-GPU DDP training).\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides.\n",
        "    with open(hparams_file) as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Create dataset objects \"train\", \"valid\", and \"test\".\n",
        "\n",
        "\n",
        "\n",
        "    datasets = dataio_prep(hparams)\n",
        "\n",
        "    hparams[\"ssl_model\"] = hparams[\"ssl_model\"].to(device=run_opts[\"device\"])\n",
        "    # freeze the feature extractor part when unfreezing\n",
        "    if not hparams[\"freeze_ssl\"] and hparams[\"freeze_ssl_conv\"]:\n",
        "        hparams[\"ssl_model\"].model.feature_extractor._freeze_parameters()\n",
        "\n",
        "    # Initialize the Brain object to prepare for mask training.\n",
        "    emo_id_brain = EmoIdBrain(\n",
        "        modules=hparams[\"modules\"],\n",
        "        opt_class=hparams[\"opt_class\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # The `fit()` method iterates the training loop, calling the methods\n",
        "    # necessary to update the parameters of the model. Since all objects\n",
        "    # with changing state are managed by the Checkpointer, training can be\n",
        "    # stopped at any point, and will be resumed on next call.\n",
        "    emo_id_brain.fit(\n",
        "        epoch_counter=emo_id_brain.hparams.epoch_counter,\n",
        "        train_set=datasets[\"train\"],\n",
        "        valid_set=datasets[\"valid\"],\n",
        "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )\n",
        "\n",
        "    # Load the best checkpoint for evaluation\n",
        "    test_stats = emo_id_brain.evaluate(\n",
        "        test_set=datasets[\"test\"],\n",
        "        min_key=\"error_rate\",\n",
        "        test_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the following code** to train the model with wav2vec:"
      ],
      "metadata": {
        "id": "g9qmIPP6mR73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zx3p1jWfHo_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3fd2b7-c0a0-4d04-9c2e-3ebe110a3114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-17 02:43:10.463850: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742179390.490822    4034 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742179390.502171    4034 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-17 02:43:10.528660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.84k/1.84k [00:00<00:00, 9.82MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "pytorch_model.bin: 100% 380M/380M [00:01<00:00, 218MB/s]\n",
            "preprocessor_config.json: 100% 159/159 [00:00<00:00, 1.12MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "speechbrain.lobes.models.huggingface_transformers.wav2vec2 - wav2vec 2.0 feature extractor is frozen.\n",
            "speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
            "speechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: /content/results/train_with_wav2vec2/1993\n",
            "speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n",
            "model.safetensors: 100% 380M/380M [00:02<00:00, 165MB/s]\n",
            "speechbrain.core - Gradscaler enabled: False. Using precision: fp32.\n",
            "/usr/local/lib/python3.11/dist-packages/speechbrain/core.py:798: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=gradscaler_enabled)\n",
            "speechbrain.core - EmoIdBrain Model Statistics:\n",
            "* Total Number of Trainable Parameters: 90.2M\n",
            "* Total Number of Parameters: 94.4M\n",
            "* Trainable Parameters represent 95.5491% of the total size.\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 100/100 [00:36<00:00,  2.74it/s, train_loss=0.447]\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 13/13 [00:01<00:00,  6.55it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 4.47e-01 - valid loss: 2.35e-01, valid error_rate: 8.00e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-43-58+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 100/100 [00:36<00:00,  2.76it/s, train_loss=0.208]\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 13/13 [00:01<00:00,  6.65it/s]\n",
            "speechbrain.utils.train_logger - Epoch: 2, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 2.08e-01 - valid loss: 2.02e-01, valid error_rate: 6.00e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-44-42+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-43-58+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 100/100 [00:36<00:00,  2.71it/s, train_loss=0.145]\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 13/13 [00:01<00:00,  6.58it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 1e-05 to 9e-06\n",
            "speechbrain.utils.train_logger - Epoch: 3, lr: 1.00e-04, ssl_lr: 1.00e-05 - train loss: 1.45e-01 - valid loss: 1.52e-01, valid error_rate: 6.00e-02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-45-30+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-44-42+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 100/100 [00:37<00:00,  2.67it/s, train_loss=0.127]\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 13/13 [00:02<00:00,  6.06it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-05 to 8.1e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 9e-06 to 8.1e-06\n",
            "speechbrain.utils.train_logger - Epoch: 4, lr: 9.00e-05, ssl_lr: 9.00e-06 - train loss: 1.27e-01 - valid loss: 4.70e-01, valid error_rate: 1.20e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-46-13+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 100/100 [00:37<00:00,  2.64it/s, train_loss=0.0241]\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 13/13 [00:02<00:00,  6.46it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-05 to 7.3e-05\n",
            "speechbrain.nnet.schedulers - Changing lr from 8.1e-06 to 7.3e-06\n",
            "speechbrain.utils.train_logger - Epoch: 5, lr: 8.10e-05, ssl_lr: 8.10e-06 - train loss: 2.41e-02 - valid loss: 8.44e-01, valid error_rate: 1.60e-01\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-46-58+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-46-13+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from /content/results/train_with_wav2vec2/1993/save/CKPT+2025-03-17+02-45-30+00\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "speechbrain.dataio.encoder - CategoricalEncoder.expect_len was never called: assuming category count of 2 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n",
            "100% 13/13 [00:02<00:00,  6.23it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 3 - test loss: 3.90e-01, test error_rate: 1.00e-01\n"
          ]
        }
      ],
      "source": [
        "!rm -rf ./results\n",
        "\n",
        "!python train.py hparams_wav2vec.yaml --data_folder='/content' --device='cuda:0' --number_of_epochs=5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything goes well, you should see training logs similar to these:\n",
        "\n",
        "\n",
        "```\n",
        "Epoch: 1, lr: 1.00e-04, sslmodel_lr: 1.00e-05 - train loss: 4.85e-01 - valid loss: 3.36e-01, valid error_rate: 1.20e-01\n",
        "Epoch: 2, lr: 1.00e-04, sslmodel_lr: 1.00e-05 - train loss: 2.47e-01 - valid loss: 4.47e-01, valid error_rate: 1.20e-01\n",
        "Epoch: 3, lr: 9.00e-05, sslmodel_lr: 9.00e-06 - train loss: 1.68e-01 - valid loss: 4.75e-01, valid error_rate: 1.00e-01\n",
        "Epoch: 4, lr: 9.00e-05, sslmodel_lr: 9.00e-06 - train loss: 8.12e-02 - valid loss: 5.06e-01, valid error_rate: 1.20e-01\n",
        "Epoch: 5, lr: 8.10e-05, sslmodel_lr: 8.10e-06 - train loss: 4.62e-02 - valid loss: 4.61e-01, valid error_rate: 8.00e-02\n",
        "Epoch loaded: 5 - test loss: 3.28e-01, test error_rate: 8.00e-02\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Note that, despite training the model with a smaller number of epochs, the performance is much better. Self-supervised models learn general and robust features that can be useful for many speech processing tasks. Depending on the random seed, you can see test errors between 12-4% (there is a lot of variability due to the small datasets used)."
      ],
      "metadata": {
        "id": "GoFQroHtbq1L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4W3R-OpgolS"
      },
      "source": [
        "In summary, we reached error rates within this range:\n",
        "\n",
        "| Model        | Errors(\\%)    \n",
        "|--------------|-----------|\n",
        "| Fbanks | 28-38\\%      |\n",
        "| Wav2Vec2 | 4-12\\%      |\n",
        "\n",
        "\n",
        "This lab should have helped you become more familiar with the process of pretraining and fine-tuning a model for speech processing. The techniques you learned here can be applied to other classification tasks, as well as sequence-to-sequence tasks like speech recognition, which we will cover in our next lecture.\n",
        "\n",
        "That all!"
      ]
    }
  ]
}