{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhV7Sj9wOMTE"
      },
      "source": [
        "# **Week 8: Lab Exercises for COMP499/691 Conversational AI**\n",
        "\n",
        "This lab focuses on end-to-end speech recognizer. The aim is to help you become more comfortable with implementing a speech recognition system. Throughout the lab, we will be exploring various strategies and methods to achieve this goal.\n",
        "\n",
        "## **Task Description**\n",
        "\n",
        "This time, our focus is on Speech recognition, also known as automatic speech recognition(ASR) to process human speech into a written format. We feed a speech signal spoken in Serbian language into the model to generate the transcription for that.\n",
        "\n",
        "Let's first download the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT4eKurD05sq",
        "outputId": "77a3708f-44f6-4ebe-96d2-ea30d0662e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-26 21:38:28--  https://www.dropbox.com/scl/fi/yfs6xh3yhmukg6itiahoy/cv-corpus-12.0-2022-12-07-sr.tar.gz?rlkey=wv9syqbh8fsv6db5d6q6uznor\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc95c25e276e11dfa804a653f86f.dl.dropboxusercontent.com/cd/0/inline/CmpjQSFwCHaNbvcpXF6kJlluFTSoPaohaCVvKw43nuXj8U5aRWWO1RYLFh3aQlrFh-3YVSAAAwDupgrNryk_DPm65AejnKjaVrMqvQlTon-IwIXFZutgHJa96J55us1GNEEWwMSmr5ZTN-SlcluPNYLG/file# [following]\n",
            "--2025-03-26 21:38:29--  https://uc95c25e276e11dfa804a653f86f.dl.dropboxusercontent.com/cd/0/inline/CmpjQSFwCHaNbvcpXF6kJlluFTSoPaohaCVvKw43nuXj8U5aRWWO1RYLFh3aQlrFh-3YVSAAAwDupgrNryk_DPm65AejnKjaVrMqvQlTon-IwIXFZutgHJa96J55us1GNEEWwMSmr5ZTN-SlcluPNYLG/file\n",
            "Resolving uc95c25e276e11dfa804a653f86f.dl.dropboxusercontent.com (uc95c25e276e11dfa804a653f86f.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6035:15::a27d:550f\n",
            "Connecting to uc95c25e276e11dfa804a653f86f.dl.dropboxusercontent.com (uc95c25e276e11dfa804a653f86f.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CmridxSBR18dZRcQqQdQZHEoAzQeFn4WFz1ubzG2vI9osBpuDJd8y-I9Rup4umVV7q-LIA7LcD8WqBim5_5DI6DMhkENt5qq2yuqNWeCn48ZHl7iOFvX2gkOGeAPFhkGdKEynwjjFFGGpIoTDQjwf4RiadctD4bFwdAyfmtulUXMN_ivlAg4c6YobFKTobkycnufEY97Al0TVf_jB_TStSKABnofIY426OE0KhyEHq6io8je78EFirjLm7keHkUEaPnuCiaKEAUZm8VEFzrXM_p5s7aEzaXWte4GULAPRGCkwBStWyr0x9oKB1hvOlX5qDp3PY51fskESi-2AeGijhySwcazmunpRAtcLBHBU-GJJK30oB-53fTGSwHL0c9GmLU/file [following]\n",
            "--2025-03-26 21:38:30--  https://uc95c25e276e11dfa804a653f86f.dl.dropboxusercontent.com/cd/0/inline2/CmridxSBR18dZRcQqQdQZHEoAzQeFn4WFz1ubzG2vI9osBpuDJd8y-I9Rup4umVV7q-LIA7LcD8WqBim5_5DI6DMhkENt5qq2yuqNWeCn48ZHl7iOFvX2gkOGeAPFhkGdKEynwjjFFGGpIoTDQjwf4RiadctD4bFwdAyfmtulUXMN_ivlAg4c6YobFKTobkycnufEY97Al0TVf_jB_TStSKABnofIY426OE0KhyEHq6io8je78EFirjLm7keHkUEaPnuCiaKEAUZm8VEFzrXM_p5s7aEzaXWte4GULAPRGCkwBStWyr0x9oKB1hvOlX5qDp3PY51fskESi-2AeGijhySwcazmunpRAtcLBHBU-GJJK30oB-53fTGSwHL0c9GmLU/file\n",
            "Reusing existing connection to uc95c25e276e11dfa804a653f86f.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58901938 (56M) [application/octet-stream]\n",
            "Saving to: ‘cv-corpus-sr.tar.gz’\n",
            "\n",
            "cv-corpus-sr.tar.gz 100%[===================>]  56.17M  15.8MB/s    in 3.6s    \n",
            "\n",
            "2025-03-26 21:38:34 (15.8 MB/s) - ‘cv-corpus-sr.tar.gz’ saved [58901938/58901938]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O cv-corpus-sr.tar.gz https://www.dropbox.com/scl/fi/yfs6xh3yhmukg6itiahoy/cv-corpus-12.0-2022-12-07-sr.tar.gz?rlkey=wv9syqbh8fsv6db5d6q6uznor&dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPvlmOYePgAI"
      },
      "source": [
        "We can now uncompress it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JrPMmW8F55Aa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!tar -zxf cv-corpus-sr.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoxE7nA6Pj-D"
      },
      "source": [
        "We also have to install the needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hu55IvWKaHz4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/speechbrain.git\n",
        "%cd speechbrain\n",
        "!pip install -r requirements.txt\n",
        "!pip install .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-oauBTvPqja"
      },
      "source": [
        "## **Exercise 1: Data Preparation**\n",
        "\n",
        "If you inspect the data stored in `/content/cv-corpus-12.0-2022-12-07/sr` you will see that there exist three tsv files ( train, valid, and dev). These tsv files contain the following information:\n",
        "\n",
        "-   client_id,\tpath, sentence,\tup_votes,\tdown_votes,\tage,\tgender,\taccents,\tlocale,\tsegment.\n",
        "\n",
        "There is a clips folder that contains all audio files.\n",
        "\n",
        "For the ASR task, you need to go over each tsv file and extract the necessary information. Specifically, you need to get the path to the audio, the transcription(sentence) and the unique_id (you could use the name of the audio file as the unique identifier).\n",
        "\n",
        "\n",
        "As you can see, we have a hierarchical structure in which the first key is a **unique identifier** of the spoken sentence. You could use  the name of the audio file as the unique identifier.\n",
        "Then, we specify all the fields that are needed for the task addressed. For instance, we report the **path of the speech recording**, its **duration** in seconds (needed if we wanna sort the sentences before creating the mini-batches), and the **sequence of words** uttered in the given recording.\n",
        "\n",
        "Also, since the ASR task could be computationally heavy, we only use a sub-sample of the data. Therefore, you need to get only 1 , 1/9 and 1/6 hours of data for training, validation, and test respectively. You could sample that using the sum of the calculated duration for each audio.\n",
        "\n",
        "**Write the code for preparing the JSON data-manifest files**.You have to create 3 JSON files:\n",
        "- 'train.json'\n",
        "- 'valid.json'\n",
        "- 'test.json'\n",
        "\n",
        "They should be formatted in the following way:\n",
        "\n",
        "\n",
        "**train.json**\n",
        "```\n",
        "{\n",
        "  \"common_voice_sr_26981130\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/clips/common_voice_sr_26981130.mp3\",\n",
        "    \"duration\": 3.168,\n",
        "    \"words\": \"То све доказује.\"\n",
        "  },\n",
        "  \"common_voice_sr_26981131\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/common_voice_sr_26981131.mp3\",\n",
        "    \"duration\": 2.988,\n",
        "    \"words\": \"А не може.\"\n",
        "  },\n",
        "  \"common_voice_sr_26981132\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/clips/common_voice_sr_26981132.mp3\",\n",
        "    \"duration\": 2.736,\n",
        "    \"words\": \"Он је пријатељ.\"\n",
        "  },\n",
        "....\n",
        "\n",
        "```\n",
        "\n",
        "**valid.json**\n",
        "```\n",
        "{\n",
        "  \"common_voice_sr_35090117\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/clips/common_voice_sr_35090117.mp3\",\n",
        "    \"duration\": 2.988,\n",
        "    \"words\": \"Па не знам.\"\n",
        "  },\n",
        "  \"common_voice_sr_35090119\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/clips/common_voice_sr_35090119.mp3\",\n",
        "    \"duration\": 2.556,\n",
        "    \"words\": \"Како да нисам.\"\n",
        "  },\n",
        "  \"common_voice_sr_35090120\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/clips/common_voice_sr_35090120.mp3\",\n",
        "    \"duration\": 2.376,\n",
        "    \"words\": \"За пет минута.\"\n",
        "  },\n",
        "....\n",
        "```\n",
        "\n",
        "**test.json**\n",
        "\n",
        "```\n",
        "{\n",
        "  \"common_voice_sr_35087568\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/clips/common_voice_sr_35087568.mp3\",\n",
        "    \"duration\": 3.276,\n",
        "    \"words\": \"Ја сам сам!\"\n",
        "  },\n",
        "  \"common_voice_sr_27607049\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/clips/common_voice_sr_27607049.mp3\",\n",
        "    \"duration\": 3.636,\n",
        "    \"words\": \"Реци и мени!\"\n",
        "  },\n",
        "  \"common_voice_sr_27607050\": {\n",
        "    \"path\": \"/content/cv-corpus-12.0-2022-12-07/sr/clips/common_voice_sr_27607050.mp3\",\n",
        "    \"duration\": 3.42,\n",
        "    \"words\": \"Хајде, хајде тамо.\"\n",
        "  },\n",
        "....\n",
        "```\n",
        "\n",
        "**Suggestions:**\n",
        "- You can get the number of samples of each wave with torchaudio.info. You have to compute the duration in seconds by diving it for the sampling frequency.\n",
        "- Since training could be time-consuming, you could start with a tiny amount of data for debugging. And when you are sure about your implementation, you could run it on the original dataset to get the final result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F3CGGZPQ7xC0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import torchaudio\n",
        "from tqdm.contrib import tzip\n",
        "import re\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "\n",
        "# Create the data-manifest files\n",
        "def create_json(tsv_file,data_folder,json_file,csv_file,max_duration):\n",
        "  # Your code here\n",
        "\n",
        "  json_dict = {}\n",
        "\n",
        "  df = pd.read_csv(tsv_file, sep='\\t')\n",
        "  total_duration = 0.0\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "\n",
        "    path = data_folder+\"/clips/\"+row['path']\n",
        "    audioinfo = torchaudio.info(path)\n",
        "    duration = audioinfo.num_frames / audioinfo.sample_rate\n",
        "    words = row['sentence']\n",
        "\n",
        "    uttid = row['path'].split(\".\")[0]\n",
        "\n",
        "    if total_duration + duration <= max_duration:\n",
        "      total_duration += duration\n",
        "\n",
        "      # Create entry for this utterance\n",
        "      json_dict[uttid] = {\n",
        "              \"path\": path,\n",
        "              \"duration\": duration,\n",
        "              \"words\": words,\n",
        "      }\n",
        "\n",
        "      # Writing the dictionary to the json file\n",
        "      with open(json_file, mode=\"w\", encoding=\"utf-8\") as json_f:\n",
        "        json.dump(json_dict, json_f,ensure_ascii=False, indent=2)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "\n",
        "# Set up data folder\n",
        "data_folder='/content/cv-corpus-12.0-2022-12-07/sr'\n",
        "max_durations=[3600,400,600] # Total durations in seconds for train, valid, test.\n",
        "\n",
        "# Create json files\n",
        "create_json(data_folder+'/train.tsv',data_folder,'train.json','train.csv',max_durations[0])\n",
        "create_json(data_folder+'/dev.tsv',data_folder,'valid.json','dev.csv',max_durations[1])\n",
        "create_json(data_folder+'/test.tsv',data_folder,'test.json','test.csv',max_durations[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1nc8N_er5j"
      },
      "source": [
        "## **Exercise 2: Speech recognition with Wav2vec + CTC Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU05LxApe6Xu"
      },
      "source": [
        "You have to implement the following model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsyfblcqw71s"
      },
      "source": [
        "![wav2vec.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDz2VcAlhD_f"
      },
      "source": [
        "For wav2vec2, you should use  https://huggingface.co/facebook/wav2vec2-large-xlsr-53 for the pretrained model and finetune it. In order to be able to use this model, you need to have a huggingface access token and log-in using this token with the following code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "For the encoder,\n",
        "\n",
        "- linear1 : (wav2vec_output_dim, 1024)\n",
        "- BatchNorm1d\n",
        "- LeakyReLU\n",
        "- Dropout : 0.15\n",
        "- linear2:  ( 1024, 1024)\n",
        "- BatchNorm1d\n",
        "- LeakyReLU\n",
        "- Dropout : 0.15\n",
        "- linear3 :( 1024, 1024)\n",
        "- BatchNorm1d\n",
        "- LeakyReLU\n",
        "\n",
        "For CTC Linear, you could use a linear layer which converts the previous layer output to output_neurons.\n",
        "\n",
        "For CTC Loss, you should use ctc_loss from speechbrain.\n",
        "\n",
        "**Important**: Use ``warmup_steps: 25`` and ``output_neurons: 500``\n",
        "\n",
        "You can draw inspiration from existing speechbrain recipes. For instance take a look at the Commonvoice recipes available [here](https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice/ASR/CTC). See in particular, `train_with_wav2vec.py` and `hparams/train_*_with_wav2vec.yaml`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "03593be6841e40b184dbea8c1f0ddec7",
            "7bfeee9849bd4466a63a847afd390ea8",
            "733bbad1adcb43e883be259c583627b9",
            "f0fd2e9465ef4a4ca0aa75952fd29870",
            "0834dab8cd8841c2b0f28c39088ab774",
            "77016ab67dec42079a1a34974fb62339",
            "ae836af6d5ce4b9684be36a77ab10d48",
            "00b0f6eeef7f4d7e927d09e59235a5d9",
            "9dedb09af1684e3ba9ffbbfcbea2dc2b",
            "8e9e5dc25fd44728afd446f334706710",
            "303a1bbb8296463bbf0b025c54d80b77",
            "3039120518c84a3487dc314c9bafa582",
            "91d19be416a74c36a7d87ebd1f8a0b4a",
            "250c7868d83a46c188279dfc57b0a082",
            "e3f0b8ca76d940e5804b88f12d4acfaa",
            "0baf9d9792ae4fe29d853c331bf90e4f",
            "e4d6fec0010643eb8bea9f4921934b8f",
            "70af752d2ee648cea74f29915a2814fe",
            "e0e7ef77f95f49a4a068da8edfe48d6c",
            "5b90aecf8c104eef866ba6fbab336462"
          ]
        },
        "id": "UpkBc7sFGJ2t",
        "outputId": "94300c90-e582-4b15-c203-09b25ba4288e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03593be6841e40b184dbea8c1f0ddec7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHigjo_S9LU6"
      },
      "source": [
        "**Write the code for the hyperparameters**:\n",
        "\n",
        "\n",
        "You need to fill specified parts in the hparams file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Oj4sWoMbw_2",
        "outputId": "c295588e-fbce-4b98-b641-0282d6aedbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting hparams_sr_wav2vec.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file hparams_sr_wav2vec.yaml\n",
        "\n",
        "# ################################\n",
        "# Model: wav2vec2 + DNN + CTC\n",
        "# Augmentation: SpecAugment\n",
        "# Authors: Titouan Parcollet 2021\n",
        "# ################################\n",
        "\n",
        "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
        "seed: 1234\n",
        "__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n",
        "output_folder: !ref results/wav2vec2_ctc_rw/<seed>\n",
        "test_wer_file: !ref <output_folder>/wer_test.txt\n",
        "save_folder: !ref <output_folder>/save\n",
        "train_log: !ref <output_folder>/train_log.txt\n",
        "\n",
        "# URL for the biggest Fairseq multilingual\n",
        "wav2vec2_hub: facebook/wav2vec2-large-xlsr-53\n",
        "wav2vec2_folder: !ref <save_folder>/wav2vec2_checkpoint\n",
        "\n",
        "# Data files\n",
        "data_folder: !PLACEHOLDER  # e.g, /localscratch/cv-corpus-5.1-2020-06-22/fr\n",
        "# train_tsv_file: !ref <data_folder>/train.tsv  # Standard CommonVoice .tsv files\n",
        "# dev_tsv_file: !ref <data_folder>/dev.tsv  # Standard CommonVoice .tsv files\n",
        "# test_tsv_file: !ref <data_folder>/test.tsv  # Standard CommonVoice .tsv files\n",
        "accented_letters: True\n",
        "language: sr\n",
        "train_csv: !ref train.csv\n",
        "valid_csv: !ref dev.csv\n",
        "test_csv: !ref test.csv\n",
        "skip_prep: False # Skip data preparation\n",
        "\n",
        "# We remove utterance slonger than 10s in the train/dev/test sets as\n",
        "# longer sentences certainly correspond to \"open microphones\".\n",
        "avoid_if_longer_than: 10.0\n",
        "\n",
        "####################### Training Parameters ####################################\n",
        "\n",
        "number_of_epochs: 30\n",
        "lr: 1.0\n",
        "lr_wav2vec: 0.0001\n",
        "sorting: ascending\n",
        "precision: fp32 # bf16, fp16 or fp32\n",
        "sample_rate: 16000\n",
        "ckpt_interval_minutes: 30 # save checkpoint every N min\n",
        "\n",
        "\n",
        "# With data_parallel batch_size is split into N jobs\n",
        "# With DDP batch_size is multiplied by N jobs\n",
        "# Must be 6 per GPU to fit 32GB of VRAM\n",
        "dynamic_batching: False\n",
        "batch_size: 12\n",
        "test_batch_size: 4\n",
        "\n",
        "dataloader_options:\n",
        "    batch_size: !ref <batch_size>\n",
        "    num_workers: 6\n",
        "test_dataloader_options:\n",
        "    batch_size: !ref <test_batch_size>\n",
        "    num_workers: 6\n",
        "\n",
        "# BPE parameters\n",
        "token_type: unigram  # [\"unigram\", \"bpe\", \"char\"]\n",
        "character_coverage: 1.0\n",
        "\n",
        "####################### Model Parameters #######################################\n",
        "# activation: !name:torch.nn.LeakyReLU\n",
        "wav2vec_output_dim: 1024\n",
        "dnn_neurons: 1024\n",
        "freeze_wav2vec: False\n",
        "freeze_feature_extractor: False\n",
        "dropout: 0.15\n",
        "warmup_steps: 25\n",
        "\n",
        "# Outputs\n",
        "output_neurons: 500  # BPE size, index(blank/eos/bos) = 0\n",
        "\n",
        "# Decoding parameters\n",
        "# Be sure that the bos and eos index match with the BPEs ones\n",
        "blank_index: 0\n",
        "bos_index: 1\n",
        "eos_index: 2\n",
        "\n",
        "# Decoding parameters\n",
        "test_beam_search:\n",
        "    blank_index: !ref <blank_index>\n",
        "    beam_size: 100\n",
        "    beam_prune_logp: -12.0\n",
        "    token_prune_min_logp: -1.2\n",
        "    prune_history: True\n",
        "    topk: 1\n",
        "    alpha: 1.0\n",
        "    beta: 0.5\n",
        "    # To use n-gram LM for decoding, follow steps in README.md.\n",
        "    # kenlm_model_path: none\n",
        "#\n",
        "# Functions and classes\n",
        "#\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "\n",
        "############################## Augmentations ###################################\n",
        "\n",
        "# Speed perturbation\n",
        "speed_perturb: !new:speechbrain.augment.time_domain.SpeedPerturb\n",
        "    orig_freq: !ref <sample_rate>\n",
        "    speeds: [95, 100, 105]\n",
        "\n",
        "# Frequency drop: randomly drops a number of frequency bands to zero.\n",
        "drop_freq: !new:speechbrain.augment.time_domain.DropFreq\n",
        "    drop_freq_low: 0\n",
        "    drop_freq_high: 1\n",
        "    drop_freq_count_low: 1\n",
        "    drop_freq_count_high: 3\n",
        "    drop_freq_width: 0.05\n",
        "\n",
        "# Time drop: randomly drops a number of temporal chunks.\n",
        "drop_chunk: !new:speechbrain.augment.time_domain.DropChunk\n",
        "    drop_length_low: 1000\n",
        "    drop_length_high: 2000\n",
        "    drop_count_low: 1\n",
        "    drop_count_high: 5\n",
        "\n",
        "# Augmenter: Combines previously defined augmentations to perform data augmentation\n",
        "wav_augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    min_augmentations: 3\n",
        "    max_augmentations: 3\n",
        "    augment_prob: 1.0\n",
        "    augmentations: [\n",
        "        !ref <speed_perturb>,\n",
        "        !ref <drop_freq>,\n",
        "        !ref <drop_chunk>]\n",
        "\n",
        "############################## Models ##########################################\n",
        "\n",
        "enc: !new:speechbrain.nnet.containers.Sequential\n",
        "    input_shape: [null, null, !ref <wav2vec_output_dim>]\n",
        "    linear1: !name:speechbrain.nnet.linear.Linear\n",
        "        n_neurons: !ref <dnn_neurons>\n",
        "        bias: True\n",
        "    bn1: !name:speechbrain.nnet.normalization.BatchNorm1d\n",
        "    activation: !new:torch.nn.LeakyReLU\n",
        "    drop: !new:torch.nn.Dropout\n",
        "        p: !ref <dropout>\n",
        "    linear2: !name:speechbrain.nnet.linear.Linear\n",
        "        n_neurons: !ref <dnn_neurons>\n",
        "        bias: True\n",
        "    bn2: !name:speechbrain.nnet.normalization.BatchNorm1d\n",
        "    activation2: !new:torch.nn.LeakyReLU\n",
        "    drop2: !new:torch.nn.Dropout\n",
        "        p: !ref <dropout>\n",
        "    linear3: !name:speechbrain.nnet.linear.Linear\n",
        "        n_neurons: !ref <dnn_neurons>\n",
        "        bias: True\n",
        "    bn3: !name:speechbrain.nnet.normalization.BatchNorm1d\n",
        "    activation3: !new:torch.nn.LeakyReLU\n",
        "\n",
        "wav2vec2: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2\n",
        "    source: !ref <wav2vec2_hub>\n",
        "    output_norm: True\n",
        "    freeze: !ref <freeze_wav2vec>\n",
        "    freeze_feature_extractor: !ref <freeze_feature_extractor>\n",
        "    save_path: !ref <wav2vec2_folder>\n",
        "\n",
        "#####\n",
        "# Uncomment this block if you prefer to use a Fairseq pretrained model instead\n",
        "# of a HuggingFace one. Here, we provide an URL that is obtained from the\n",
        "# Fairseq github for the multilingual XLSR.\n",
        "#\n",
        "#wav2vec2_url: https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt\n",
        "#wav2vec2: !new:speechbrain.lobes.models.fairseq_wav2vec.FairseqWav2Vec2\n",
        "#    pretrained_path: !ref <wav2vec2_url>\n",
        "#    output_norm: True\n",
        "#    freeze: False\n",
        "#    save_path: !ref <save_folder>/wav2vec2_checkpoint/model.pt\n",
        "#####\n",
        "\n",
        "\n",
        "ctc_lin: !new:speechbrain.nnet.linear.Linear\n",
        "    input_size: !ref <dnn_neurons>\n",
        "    n_neurons: !ref <output_neurons>\n",
        "\n",
        "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
        "    apply_log: True\n",
        "\n",
        "ctc_cost: !name:speechbrain.nnet.losses.ctc_loss\n",
        "    blank_index: !ref <blank_index>\n",
        "\n",
        "modules:\n",
        "    wav2vec2: !ref <wav2vec2>\n",
        "    enc: !ref <enc>\n",
        "    ctc_lin: !ref <ctc_lin>\n",
        "\n",
        "model: !new:torch.nn.ModuleList\n",
        "    - [!ref <enc>, !ref <ctc_lin>]\n",
        "\n",
        "model_opt_class: !name:torch.optim.Adadelta\n",
        "    lr: !ref <lr>\n",
        "    rho: 0.95\n",
        "    eps: 1.e-8\n",
        "\n",
        "wav2vec_opt_class: !name:torch.optim.Adam\n",
        "    lr: !ref <lr_wav2vec>\n",
        "\n",
        "lr_annealing_model: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.8\n",
        "    patient: 0\n",
        "\n",
        "lr_annealing_wav2vec: !new:speechbrain.nnet.schedulers.NewBobScheduler\n",
        "    initial_value: !ref <lr_wav2vec>\n",
        "    improvement_threshold: 0.0025\n",
        "    annealing_factor: 0.9\n",
        "    patient: 0\n",
        "\n",
        "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
        "    checkpoints_dir: !ref <save_folder>\n",
        "    recoverables:\n",
        "        wav2vec2: !ref <wav2vec2>\n",
        "        model: !ref <model>\n",
        "        scheduler_model: !ref <lr_annealing_model>\n",
        "        scheduler_wav2vec: !ref <lr_annealing_wav2vec>\n",
        "        counter: !ref <epoch_counter>\n",
        "\n",
        "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
        "    save_file: !ref <train_log>\n",
        "\n",
        "error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
        "\n",
        "cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
        "    split_tokens: True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo0EHAe1ra1B"
      },
      "source": [
        "**Write the code for the training script**:\n",
        "\n",
        " You need to complete compute_forward, compute_objectives and dataio_prepare functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZCxZujihU50",
        "outputId": "84c328d1-a5e0-4685-d33c-ea4605d2cfee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting train_with_wav2vec.py\n"
          ]
        }
      ],
      "source": [
        "%%file train_with_wav2vec.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Recipe for training a sequence-to-sequence ASR system with CommonVoice.\n",
        "The system employs a wav2vec2 encoder and a CTC decoder.\n",
        "Decoding is performed with greedy decoding (will be extended to beam search).\n",
        "\n",
        "To run this recipe, do the following:\n",
        "> python train_with_wav2vec2.py hparams/train_with_wav2vec2.yaml\n",
        "\n",
        "With the default hyperparameters, the system employs a pretrained wav2vec2 encoder.\n",
        "The wav2vec2 model is pretrained following the model given in the hparams file.\n",
        "It may be dependent on the language.\n",
        "\n",
        "The neural network is trained with CTC on sub-word units estimated with\n",
        "Byte Pairwise Encoding (BPE).\n",
        "\n",
        "The experiment file is flexible enough to support a large variety of\n",
        "different systems. By properly changing the parameter files, you can try\n",
        "different encoders, decoders, tokens (e.g, characters instead of BPE),\n",
        "training languages (all CommonVoice languages), and many\n",
        "other possible variations.\n",
        "\n",
        "Authors\n",
        " * Titouan Parcollet 2021\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torchaudio\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "\n",
        "import speechbrain as sb\n",
        "from speechbrain.tokenizers.SentencePiece import SentencePiece\n",
        "from speechbrain.utils.data_utils import undo_padding\n",
        "\n",
        "\n",
        "from speechbrain.utils.distributed import if_main_process,run_on_main\n",
        "from speechbrain.utils.logger import get_logger\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm.contrib import tzip\n",
        "from speechbrain.dataio.dataio import read_audio\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "\n",
        "class ASR(sb.core.Brain):\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
        "\n",
        "\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, wav_lens = batch.sig\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "\n",
        "        # Optional waveform augmentation\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            wavs, wav_lens = self.hparams.wav_augment(wavs, wav_lens)\n",
        "\n",
        "        # Forward pass: Wav2Vec2 → DNN → CTC linear → log-softmax\n",
        "        feats = self.modules.wav2vec2(wavs, wav_lens)\n",
        "        x = self.modules.enc(feats)\n",
        "        logits = self.modules.ctc_lin(x)\n",
        "        p_ctc = self.hparams.log_softmax(logits)\n",
        "\n",
        "        p_tokens = None\n",
        "        # During VALID or TEST, decode with greedy or beam search\n",
        "        if stage == sb.Stage.VALID:\n",
        "            p_tokens = sb.decoders.ctc_greedy_decode(\n",
        "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
        "            )\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            # If using beam search:\n",
        "            p_tokens = test_searcher(p_ctc, wav_lens)\n",
        "\n",
        "        return p_ctc, wav_lens, p_tokens\n",
        "\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss (CTC) given predictions and targets.\"\"\"\n",
        "\n",
        "        p_ctc, wav_lens, p_tokens = predictions\n",
        "\n",
        "        ids = batch.id\n",
        "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
        "        tokens, tokens_lens = batch.tokens\n",
        "\n",
        "        if stage == sb.Stage.TRAIN and hasattr(self.hparams, \"wav_augment\"):\n",
        "            tokens = self.hparams.wav_augment.replicate_labels(tokens)\n",
        "            tokens_lens = self.hparams.wav_augment.replicate_labels(tokens_lens)\n",
        "\n",
        "        loss = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)\n",
        "\n",
        "        if stage == sb.Stage.VALID:\n",
        "            # Convert token indices to words\n",
        "            predicted_words = self.tokenizer(p_tokens, task=\"decode_from_list\")\n",
        "\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            predicted_words = [hyp[0].text.split(\" \") for hyp in p_tokens]\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            # Convert indices to words\n",
        "            target_words = undo_padding(tokens, tokens_lens)\n",
        "            target_words = self.tokenizer(target_words, task=\"decode_from_list\")\n",
        "\n",
        "            self.wer_metric.append(ids, predicted_words, target_words)\n",
        "            self.cer_metric.append(ids, predicted_words, target_words)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_stage_start(self, stage, epoch):\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.cer_metric = self.hparams.cer_computer()\n",
        "            self.wer_metric = self.hparams.error_rate_computer()\n",
        "\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
        "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr_model, new_lr_model = self.hparams.lr_annealing_model(\n",
        "                stage_stats[\"loss\"]\n",
        "            )\n",
        "            old_lr_wav2vec, new_lr_wav2vec = self.hparams.lr_annealing_wav2vec(\n",
        "                stage_stats[\"loss\"]\n",
        "            )\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.model_optimizer, new_lr_model\n",
        "            )\n",
        "            if not self.hparams.wav2vec2.freeze:\n",
        "                sb.nnet.schedulers.update_learning_rate(\n",
        "                    self.wav2vec_optimizer, new_lr_wav2vec\n",
        "                )\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\n",
        "                    \"epoch\": epoch,\n",
        "                    \"lr_model\": old_lr_model,\n",
        "                    \"lr_wav2vec\": old_lr_wav2vec,\n",
        "                },\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"WER\": stage_stats[\"WER\"]},\n",
        "                min_keys=[\"WER\"],\n",
        "            )\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "            if if_main_process():\n",
        "                with open(\n",
        "                    self.hparams.test_wer_file, \"w\", encoding=\"utf-8\"\n",
        "                ) as w:\n",
        "                    self.wer_metric.write_stats(w)\n",
        "\n",
        "\n",
        "\n",
        "    def init_optimizers(self):\n",
        "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
        "\n",
        "        # If the wav2vec encoder is unfrozen, we create the optimizer\n",
        "        if not self.hparams.wav2vec2.freeze:\n",
        "            self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
        "                self.modules.wav2vec2.parameters()\n",
        "            )\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"wav2vec_opt\", self.wav2vec_optimizer\n",
        "                )\n",
        "\n",
        "        self.model_optimizer = self.hparams.model_opt_class(\n",
        "            self.hparams.model.parameters()\n",
        "        )\n",
        "\n",
        "        if self.checkpointer is not None:\n",
        "            self.checkpointer.add_recoverable(\"modelopt\", self.model_optimizer)\n",
        "\n",
        "        if not self.hparams.wav2vec2.freeze:\n",
        "            self.optimizers_dict = {\n",
        "                \"wav2vec_optimizer\": self.wav2vec_optimizer,\n",
        "                \"model_optimizer\": self.model_optimizer,\n",
        "            }\n",
        "        else:\n",
        "            self.optimizers_dict = {\"model_optimizer\": self.model_optimizer}\n",
        "\n",
        "    def freeze_optimizers(self, optimizers):\n",
        "        \"\"\"Freezes the wav2vec2 optimizer according to the warmup steps\"\"\"\n",
        "        valid_optimizers = {}\n",
        "        if not self.hparams.wav2vec2.freeze:\n",
        "            if self.optimizer_step >= self.hparams.warmup_steps:\n",
        "                valid_optimizers[\"wav2vec_optimizer\"] = optimizers[\n",
        "                    \"wav2vec_optimizer\"\n",
        "                ]\n",
        "        valid_optimizers[\"model_optimizer\"] = optimizers[\"model_optimizer\"]\n",
        "        return valid_optimizers\n",
        "\n",
        "\n",
        "# Define custom data procedure\n",
        "def dataio_prepare(hparams, tokenizer):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Define datasets\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_csv\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    if hparams[\"sorting\"] == \"ascending\":\n",
        "        # we sort training data to speed up training and get better results.\n",
        "        train_data = train_data.filtered_sorted(\n",
        "            sort_key=\"duration\",\n",
        "            key_max_value={\"duration\": hparams[\"avoid_if_longer_than\"]},\n",
        "        )\n",
        "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
        "        hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
        "\n",
        "    elif hparams[\"sorting\"] == \"descending\":\n",
        "        train_data = train_data.filtered_sorted(\n",
        "            sort_key=\"duration\",\n",
        "            reverse=True,\n",
        "            key_max_value={\"duration\": hparams[\"avoid_if_longer_than\"]},\n",
        "        )\n",
        "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
        "        hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
        "\n",
        "    elif hparams[\"sorting\"] == \"random\":\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            \"sorting must be random, ascending or descending\"\n",
        "        )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_csv\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "    # We also sort the validation data so it is faster to validate\n",
        "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
        "\n",
        "    test_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"test_csv\"],\n",
        "        replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    # We also sort the validation data so it is faster to validate\n",
        "    test_data = test_data.filtered_sorted(sort_key=\"duration\")\n",
        "\n",
        "    datasets = [train_data, valid_data, test_data]\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"path\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav):\n",
        "        info = torchaudio.info(wav)\n",
        "        sig = sb.dataio.dataio.read_audio(wav)\n",
        "        resampled = torchaudio.transforms.Resample(\n",
        "            info.sample_rate,\n",
        "            hparams[\"sample_rate\"],\n",
        "        )(sig)\n",
        "        return resampled\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wrd\")\n",
        "    @sb.utils.data_pipeline.provides(\n",
        "        \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
        "    )\n",
        "    def text_pipeline(wrd):\n",
        "        tokens_list = tokenizer.sp.encode_as_ids(wrd)\n",
        "        yield tokens_list\n",
        "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
        "        yield tokens_bos\n",
        "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
        "        yield tokens_eos\n",
        "        tokens = torch.LongTensor(tokens_list)\n",
        "        yield tokens\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets,\n",
        "        [\"id\", \"sig\", \"tokens_bos\", \"tokens_eos\", \"tokens\"],\n",
        "    )\n",
        "\n",
        "    # 5. If Dynamic Batching is used, we instantiate the needed samplers.\n",
        "    train_batch_sampler = None\n",
        "    valid_batch_sampler = None\n",
        "    if hparams[\"dynamic_batching\"]:\n",
        "        from speechbrain.dataio.sampler import DynamicBatchSampler  # noqa\n",
        "\n",
        "        dynamic_hparams_train = hparams[\"dynamic_batch_sampler_train\"]\n",
        "        dynamic_hparams_valid = hparams[\"dynamic_batch_sampler_valid\"]\n",
        "\n",
        "        train_batch_sampler = DynamicBatchSampler(\n",
        "            train_data,\n",
        "            length_func=lambda x: x[\"duration\"],\n",
        "            **dynamic_hparams_train,\n",
        "        )\n",
        "        valid_batch_sampler = DynamicBatchSampler(\n",
        "            valid_data,\n",
        "            length_func=lambda x: x[\"duration\"],\n",
        "            **dynamic_hparams_valid,\n",
        "        )\n",
        "\n",
        "    return (\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        test_data,\n",
        "        train_batch_sampler,\n",
        "        valid_batch_sampler,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# Create the data-manifest files\n",
        "def create_csv(tsv_file,data_folder,csv_file,max_duration):\n",
        "  # Your code here\n",
        "\n",
        "  csv_rows = []\n",
        "\n",
        "  df = pd.read_csv(tsv_file, sep='\\t')\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "\n",
        "    path = data_folder+\"/clips/\"+row['path']\n",
        "    audioinfo = torchaudio.info(path)\n",
        "    duration = audioinfo.num_frames / audioinfo.sample_rate\n",
        "    words = row['sentence']\n",
        "\n",
        "    uttid = row['path'].split(\".\")[0]\n",
        "\n",
        "    if duration > max_duration:\n",
        "      continue\n",
        "\n",
        "    csv_rows.append({\n",
        "            \"ID\": uttid,\n",
        "            \"path\": path,\n",
        "            \"wrd\": words,\n",
        "            \"duration\": duration\n",
        "        })\n",
        "\n",
        "\n",
        "    csv_df = pd.DataFrame(csv_rows)\n",
        "    csv_df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Set up data folder\n",
        "    data_folder='/content/cv-corpus-12.0-2022-12-07/sr'\n",
        "    max_durations=[3600,400,600] # Total durations in seconds for train, valid, test.\n",
        "\n",
        "    # Create json files\n",
        "    create_csv(data_folder+'/train.tsv',data_folder,'train.csv',max_durations[0])\n",
        "    create_csv(data_folder+'/dev.tsv',data_folder,'dev.csv',max_durations[1])\n",
        "    create_csv(data_folder+'/test.tsv',data_folder,'test.csv',max_durations[2])\n",
        "\n",
        "    # Load hyperparameters file with command-line overrides\n",
        "    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n",
        "    with open(hparams_file, encoding=\"utf-8\") as fin:\n",
        "        hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "    # create ddp_group with the right communication protocol\n",
        "    sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "    # # Dataset preparation (parsing CommonVoice)\n",
        "    # from common_voice_prepare import prepare_common_voice  # noqa\n",
        "\n",
        "    # Create experiment directory\n",
        "    sb.create_experiment_directory(\n",
        "        experiment_directory=hparams[\"output_folder\"],\n",
        "        hyperparams_to_save=hparams_file,\n",
        "        overrides=overrides,\n",
        "    )\n",
        "\n",
        "    # Defining tokenizer and loading it\n",
        "    tokenizer = SentencePiece(\n",
        "        model_dir=hparams[\"save_folder\"],\n",
        "        vocab_size=hparams[\"output_neurons\"],\n",
        "        annotation_train=hparams[\"train_csv\"],\n",
        "        annotation_read=\"wrd\",\n",
        "        model_type=hparams[\"token_type\"],\n",
        "        character_coverage=hparams[\"character_coverage\"],\n",
        "    )\n",
        "\n",
        "    # Create the datasets objects as well as tokenization and encoding :-D\n",
        "    (\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        test_data,\n",
        "        train_bsampler,\n",
        "        valid_bsampler,\n",
        "    ) = dataio_prepare(hparams, tokenizer)\n",
        "\n",
        "    # Trainer initialization\n",
        "    asr_brain = ASR(\n",
        "        modules=hparams[\"modules\"],\n",
        "        hparams=hparams,\n",
        "        run_opts=run_opts,\n",
        "        checkpointer=hparams[\"checkpointer\"],\n",
        "    )\n",
        "\n",
        "    # Adding objects to trainer.\n",
        "    asr_brain.tokenizer = tokenizer\n",
        "    vocab_list = [\n",
        "        tokenizer.sp.id_to_piece(i) for i in range(tokenizer.sp.vocab_size())\n",
        "    ]\n",
        "\n",
        "    from speechbrain.decoders.ctc import CTCBeamSearcher\n",
        "\n",
        "    test_searcher = CTCBeamSearcher(\n",
        "        **hparams[\"test_beam_search\"],\n",
        "        vocab_list=vocab_list,\n",
        "    )\n",
        "\n",
        "    # Manage dynamic batching\n",
        "    train_dataloader_opts = hparams[\"dataloader_options\"]\n",
        "    valid_dataloader_opts = hparams[\"test_dataloader_options\"]\n",
        "    if train_bsampler is not None:\n",
        "        collate_fn = None\n",
        "        if \"collate_fn\" in train_dataloader_opts:\n",
        "            collate_fn = train_dataloader_opts[\"collate_fn\"]\n",
        "\n",
        "        train_dataloader_opts = {\n",
        "            \"batch_sampler\": train_bsampler,\n",
        "            \"num_workers\": hparams[\"num_workers\"],\n",
        "        }\n",
        "\n",
        "        if collate_fn is not None:\n",
        "            train_dataloader_opts[\"collate_fn\"] = collate_fn\n",
        "\n",
        "    if valid_bsampler is not None:\n",
        "        collate_fn = None\n",
        "        if \"collate_fn\" in valid_dataloader_opts:\n",
        "            collate_fn = valid_dataloader_opts[\"collate_fn\"]\n",
        "\n",
        "        valid_dataloader_opts = {\"batch_sampler\": valid_bsampler}\n",
        "\n",
        "        if collate_fn is not None:\n",
        "            valid_dataloader_opts[\"collate_fn\"] = collate_fn\n",
        "\n",
        "    # Training\n",
        "    asr_brain.fit(\n",
        "        asr_brain.hparams.epoch_counter,\n",
        "        train_data,\n",
        "        valid_data,\n",
        "        train_loader_kwargs=train_dataloader_opts,\n",
        "        valid_loader_kwargs=valid_dataloader_opts,\n",
        "    )\n",
        "\n",
        "    # Test\n",
        "    asr_brain.evaluate(\n",
        "        test_data,\n",
        "        min_key=\"WER\",\n",
        "        test_loader_kwargs=hparams[\"test_dataloader_options\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FKIh9FJrriu"
      },
      "source": [
        "**Run the code below** to train the model\n",
        "\n",
        "It would take around 20-30 minutes on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LKJqMNPuB8L",
        "outputId": "919b207e-ed7b-46d2-d917-6cf96c72b3e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-26 21:41:35.615464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743025295.636441    6971 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743025295.642766    6971 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 21:41:35.664459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.77k/1.77k [00:00<00:00, 9.72MB/s]\n",
            "pytorch_model.bin: 100% 1.27G/1.27G [00:18<00:00, 68.9MB/s]\n",
            "preprocessor_config.json: 100% 212/212 [00:00<00:00, 1.63MB/s]\n",
            "speechbrain.utils.quirks - Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
            "speechbrain.utils.quirks - Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
            "speechbrain.core - Beginning experiment!\n",
            "speechbrain.core - Experiment folder: results/wav2vec2_ctc_rw/1986\n",
            "speechbrain.tokenizers.SentencePiece - Train tokenizer with type:unigram\n",
            "speechbrain.tokenizers.SentencePiece - Extract wrd sequences from:train.csv\n",
            "speechbrain.tokenizers.SentencePiece - Text file created at: results/wav2vec2_ctc_rw/1986/save/train.txt\n",
            "model.safetensors:   3% 41.9M/1.27G [00:00<00:20, 61.1MB/s]sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=results/wav2vec2_ctc_rw/1986/save/train.txt --model_prefix=results/wav2vec2_ctc_rw/1986/save/500_unigram --model_type=unigram --bos_id=-1 --eos_id=-1 --pad_id=-1 --unk_id=0 --max_sentencepiece_length=10 --character_coverage=1.0 --add_dummy_prefix=True --vocab_size=500\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: results/wav2vec2_ctc_rw/1986/save/train.txt\n",
            "  input_format: \n",
            "  model_prefix: results/wav2vec2_ctc_rw/1986/save/500_unigram\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 500\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 1\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 10\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: -1\n",
            "  eos_id: -1\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: results/wav2vec2_ctc_rw/1986/save/train.txt\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 1380 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=24628\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1380 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=12967\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 3169 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1380\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 1869\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 1869 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1480 obj=11.415 num_tokens=4337 num_tokens/piece=2.93041\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1304 obj=9.85532 num_tokens=4352 num_tokens/piece=3.33742\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=975 obj=10.1276 num_tokens=4658 num_tokens/piece=4.77744\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=971 obj=10.0086 num_tokens=4659 num_tokens/piece=4.79815\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=728 obj=10.5006 num_tokens=5137 num_tokens/piece=7.05632\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=728 obj=10.3703 num_tokens=5137 num_tokens/piece=7.05632\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=550 obj=11.2717 num_tokens=5734 num_tokens/piece=10.4255\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=550 obj=11.1392 num_tokens=5744 num_tokens/piece=10.4436\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: results/wav2vec2_ctc_rw/1986/save/500_unigram.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: results/wav2vec2_ctc_rw/1986/save/500_unigram.vocab\n",
            "speechbrain.tokenizers.SentencePiece - ==== Loading Tokenizer ===\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer path: results/wav2vec2_ctc_rw/1986/save/500_unigram.model\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer vocab_size: 500\n",
            "speechbrain.tokenizers.SentencePiece - Tokenizer type: unigram\n",
            "speechbrain.core - Info: precision arg from hparam file is used\n",
            "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
            "speechbrain.core - Gradscaler enabled: `False`\n",
            "speechbrain.core - Using training precision: `--precision=fp32`\n",
            "speechbrain.core - Using evaluation precision: `--eval_precision=fp32`\n",
            "speechbrain.core - ASR Model Statistics:\n",
            "* Total Number of Trainable Parameters: 319.1M\n",
            "* Total Number of Parameters: 319.1M\n",
            "* Trainable Parameters represent 100.0000% of the total size.\n",
            "model.safetensors:   9% 115M/1.27G [00:02<00:13, 84.0MB/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
            "speechbrain.utils.epoch_loop - Going into epoch 1\n",
            "model.safetensors:   9% 115M/1.27G [00:02<00:13, 84.0MB/s]\n",
            "model.safetensors:  24% 304M/1.27G [00:04<00:10, 89.5MB/s]\n",
            "  0% 0/115 [00:02<?, ?it/s, train_loss=113]\u001b[A\n",
            "model.safetensors:  28% 357M/1.27G [00:05<00:10, 88.3MB/s]\n",
            "  1% 1/115 [00:03<04:54,  2.58s/it, train_loss=111]\u001b[A\n",
            "model.safetensors:  34% 430M/1.27G [00:05<00:08, 93.9MB/s]\n",
            "  2% 2/115 [00:03<02:44,  1.45s/it, train_loss=109]\u001b[A\n",
            "model.safetensors:  39% 493M/1.27G [00:06<00:08, 93.7MB/s]\n",
            "  3% 3/115 [00:04<02:04,  1.11s/it, train_loss=105]\u001b[A\n",
            "model.safetensors:  45% 566M/1.27G [00:07<00:08, 87.3MB/s]\n",
            "  3% 4/115 [00:05<01:45,  1.05it/s, train_loss=103]\u001b[A\n",
            "model.safetensors:  50% 629M/1.27G [00:08<00:07, 91.5MB/s]\n",
            "  4% 5/115 [00:06<01:40,  1.10it/s, train_loss=97.8]\u001b[A\n",
            "model.safetensors:  55% 703M/1.27G [00:09<00:06, 87.3MB/s]\n",
            "  5% 6/115 [00:07<01:32,  1.18it/s, train_loss=93.8]\u001b[A\n",
            "model.safetensors:  59% 755M/1.27G [00:09<00:06, 75.7MB/s]\n",
            "  6% 7/115 [00:07<01:29,  1.20it/s, train_loss=89.8]\u001b[A\n",
            "model.safetensors:  63% 797M/1.27G [00:10<00:06, 76.0MB/s]\n",
            "  7% 8/115 [00:08<01:26,  1.24it/s, train_loss=87]  \u001b[A\n",
            "model.safetensors:  69% 870M/1.27G [00:11<00:04, 86.3MB/s]\n",
            "  8% 9/115 [00:09<01:24,  1.26it/s, train_loss=87.6]\u001b[A\n",
            "model.safetensors:  73% 923M/1.27G [00:11<00:04, 78.5MB/s]\n",
            "  9% 10/115 [00:10<01:23,  1.25it/s, train_loss=84.6]\u001b[A\n",
            "model.safetensors:  78% 986M/1.27G [00:12<00:03, 75.1MB/s]\n",
            " 10% 11/115 [00:10<01:20,  1.29it/s, train_loss=82.3]\u001b[A\n",
            "model.safetensors:  83% 1.05G/1.27G [00:13<00:02, 85.0MB/s]\n",
            " 10% 12/115 [00:11<01:21,  1.26it/s, train_loss=79.4]\u001b[A\n",
            "model.safetensors:  88% 1.11G/1.27G [00:14<00:02, 76.3MB/s]\n",
            " 11% 13/115 [00:12<01:20,  1.27it/s, train_loss=77.4]\u001b[A\n",
            "model.safetensors:  93% 1.18G/1.27G [00:15<00:00, 92.4MB/s]\n",
            " 12% 14/115 [00:13<01:19,  1.27it/s, train_loss=75.9]\u001b[A\n",
            "model.safetensors:  99% 1.26G/1.27G [00:16<00:00, 92.4MB/s]\n",
            " 13% 15/115 [00:14<01:20,  1.24it/s, train_loss=73.5]\u001b[A\n",
            "model.safetensors: 100% 1.27G/1.27G [00:16<00:00, 78.2MB/s]\n",
            "\n",
            " 14% 16/115 [00:14<01:21,  1.22it/s, train_loss=70.9]\u001b[A\n",
            " 15% 17/115 [00:14<01:18,  1.24it/s, train_loss=70.9]\u001b[A\n",
            " 15% 17/115 [00:15<01:18,  1.24it/s, train_loss=69.8]\u001b[A\n",
            " 16% 18/115 [00:15<01:18,  1.23it/s, train_loss=69.8]\u001b[A\n",
            " 16% 18/115 [00:16<01:18,  1.23it/s, train_loss=68.1]\u001b[A\n",
            " 17% 19/115 [00:16<01:19,  1.21it/s, train_loss=68.1]\u001b[A\n",
            " 17% 19/115 [00:17<01:19,  1.21it/s, train_loss=66.5]\u001b[A\n",
            " 17% 20/115 [00:17<01:19,  1.19it/s, train_loss=66.5]\u001b[A\n",
            " 17% 20/115 [00:18<01:19,  1.19it/s, train_loss=65.1]\u001b[A\n",
            " 18% 21/115 [00:18<01:18,  1.20it/s, train_loss=65.1]\u001b[A\n",
            " 18% 21/115 [00:19<01:18,  1.20it/s, train_loss=63.8]\u001b[A\n",
            " 19% 22/115 [00:19<01:18,  1.18it/s, train_loss=63.8]\u001b[A\n",
            " 19% 22/115 [00:20<01:18,  1.18it/s, train_loss=62.5]\u001b[A\n",
            " 20% 23/115 [00:20<01:20,  1.15it/s, train_loss=62.5]\u001b[A\n",
            " 20% 23/115 [00:20<01:20,  1.15it/s, train_loss=60.9]\u001b[A\n",
            " 21% 24/115 [00:20<01:16,  1.19it/s, train_loss=60.9]\u001b[A\n",
            " 21% 24/115 [00:21<01:16,  1.19it/s, train_loss=59.3]\u001b[A\n",
            " 22% 25/115 [00:21<01:14,  1.20it/s, train_loss=59.3]\u001b[A\n",
            " 22% 25/115 [00:22<01:14,  1.20it/s, train_loss=57.7]\u001b[A\n",
            " 23% 26/115 [00:22<01:17,  1.14it/s, train_loss=57.7]\u001b[A\n",
            " 23% 26/115 [00:23<01:17,  1.14it/s, train_loss=56.2]\u001b[A\n",
            " 23% 27/115 [00:23<01:19,  1.11it/s, train_loss=56.2]\u001b[A\n",
            " 23% 27/115 [00:24<01:19,  1.11it/s, train_loss=54.8]\u001b[A\n",
            " 24% 28/115 [00:24<01:21,  1.06it/s, train_loss=54.8]\u001b[A\n",
            " 24% 28/115 [00:25<01:21,  1.06it/s, train_loss=53.5]\u001b[A\n",
            " 25% 29/115 [00:25<01:18,  1.10it/s, train_loss=53.5]\u001b[A\n",
            " 25% 29/115 [00:26<01:18,  1.10it/s, train_loss=52.2]\u001b[A\n",
            " 26% 30/115 [00:26<01:19,  1.08it/s, train_loss=52.2]\u001b[A\n",
            " 26% 30/115 [00:27<01:19,  1.08it/s, train_loss=51]  \u001b[A\n",
            " 27% 31/115 [00:27<01:20,  1.04it/s, train_loss=51]\u001b[A\n",
            " 27% 31/115 [00:28<01:20,  1.04it/s, train_loss=49.6]\u001b[A\n",
            " 28% 32/115 [00:28<01:18,  1.06it/s, train_loss=49.6]\u001b[A\n",
            " 28% 32/115 [00:29<01:18,  1.06it/s, train_loss=48.4]\u001b[A\n",
            " 29% 33/115 [00:29<01:19,  1.03it/s, train_loss=48.4]\u001b[A\n",
            " 29% 33/115 [00:30<01:19,  1.03it/s, train_loss=47.1]\u001b[A\n",
            " 30% 34/115 [00:30<01:18,  1.04it/s, train_loss=47.1]\u001b[A\n",
            " 30% 34/115 [00:31<01:18,  1.04it/s, train_loss=46]  \u001b[A\n",
            " 30% 35/115 [00:31<01:18,  1.02it/s, train_loss=46]\u001b[A\n",
            " 30% 35/115 [00:32<01:18,  1.02it/s, train_loss=44.9]\u001b[A\n",
            " 31% 36/115 [00:32<01:17,  1.03it/s, train_loss=44.9]\u001b[A\n",
            " 31% 36/115 [00:33<01:17,  1.03it/s, train_loss=43.8]\u001b[A\n",
            " 32% 37/115 [00:33<01:14,  1.04it/s, train_loss=43.8]\u001b[A\n",
            " 32% 37/115 [00:34<01:14,  1.04it/s, train_loss=42.8]\u001b[A\n",
            " 33% 38/115 [00:34<01:15,  1.02it/s, train_loss=42.8]\u001b[A\n",
            " 33% 38/115 [00:35<01:15,  1.02it/s, train_loss=41.9]\u001b[A\n",
            " 34% 39/115 [00:35<01:13,  1.04it/s, train_loss=41.9]\u001b[A\n",
            " 34% 39/115 [00:36<01:13,  1.04it/s, train_loss=41]  \u001b[A\n",
            " 35% 40/115 [00:36<01:13,  1.02it/s, train_loss=41]\u001b[A\n",
            " 35% 40/115 [00:37<01:13,  1.02it/s, train_loss=40.1]\u001b[A\n",
            " 36% 41/115 [00:37<01:14,  1.00s/it, train_loss=40.1]\u001b[A\n",
            " 36% 41/115 [00:38<01:14,  1.00s/it, train_loss=39.3]\u001b[A\n",
            " 37% 42/115 [00:38<01:11,  1.02it/s, train_loss=39.3]\u001b[A\n",
            " 37% 42/115 [00:39<01:11,  1.02it/s, train_loss=38.5]\u001b[A\n",
            " 37% 43/115 [00:39<01:08,  1.05it/s, train_loss=38.5]\u001b[A\n",
            " 37% 43/115 [00:40<01:08,  1.05it/s, train_loss=37.8]\u001b[A\n",
            " 38% 44/115 [00:40<01:06,  1.06it/s, train_loss=37.8]\u001b[A\n",
            " 38% 44/115 [00:41<01:06,  1.06it/s, train_loss=37]  \u001b[A\n",
            " 39% 45/115 [00:41<01:08,  1.02it/s, train_loss=37]\u001b[A\n",
            " 39% 45/115 [00:42<01:08,  1.02it/s, train_loss=36.4]\u001b[A\n",
            " 40% 46/115 [00:42<01:08,  1.01it/s, train_loss=36.4]\u001b[A\n",
            " 40% 46/115 [00:43<01:08,  1.01it/s, train_loss=35.7]\u001b[A\n",
            " 41% 47/115 [00:43<01:08,  1.01s/it, train_loss=35.7]\u001b[A\n",
            " 41% 47/115 [00:44<01:08,  1.01s/it, train_loss=35.1]\u001b[A\n",
            " 42% 48/115 [00:44<01:08,  1.02s/it, train_loss=35.1]\u001b[A\n",
            " 42% 48/115 [00:45<01:08,  1.02s/it, train_loss=34.5]\u001b[A\n",
            " 43% 49/115 [00:45<01:09,  1.05s/it, train_loss=34.5]\u001b[A\n",
            " 43% 49/115 [00:46<01:09,  1.05s/it, train_loss=33.9]\u001b[A\n",
            " 43% 50/115 [00:46<01:10,  1.08s/it, train_loss=33.9]\u001b[A\n",
            " 43% 50/115 [00:47<01:10,  1.08s/it, train_loss=33.4]\u001b[A\n",
            " 44% 51/115 [00:47<01:10,  1.11s/it, train_loss=33.4]\u001b[A\n",
            " 44% 51/115 [00:48<01:10,  1.11s/it, train_loss=32.8]\u001b[A\n",
            " 45% 52/115 [00:48<01:11,  1.13s/it, train_loss=32.8]\u001b[A\n",
            " 45% 52/115 [00:49<01:11,  1.13s/it, train_loss=32.3]\u001b[A\n",
            " 46% 53/115 [00:49<01:09,  1.12s/it, train_loss=32.3]\u001b[A\n",
            " 46% 53/115 [00:50<01:09,  1.12s/it, train_loss=31.8]\u001b[A\n",
            " 47% 54/115 [00:50<01:05,  1.08s/it, train_loss=31.8]\u001b[A\n",
            " 47% 54/115 [00:52<01:05,  1.08s/it, train_loss=31.3]\u001b[A\n",
            " 48% 55/115 [00:52<01:06,  1.10s/it, train_loss=31.3]\u001b[A\n",
            " 48% 55/115 [00:53<01:06,  1.10s/it, train_loss=30.9]\u001b[A\n",
            " 49% 56/115 [00:53<01:05,  1.11s/it, train_loss=30.9]\u001b[A\n",
            " 49% 56/115 [00:54<01:05,  1.11s/it, train_loss=30.5]\u001b[A\n",
            " 50% 57/115 [00:54<01:03,  1.09s/it, train_loss=30.5]\u001b[A\n",
            " 50% 57/115 [00:55<01:03,  1.09s/it, train_loss=30]  \u001b[A\n",
            " 50% 58/115 [00:55<01:02,  1.10s/it, train_loss=30]\u001b[A\n",
            " 50% 58/115 [00:56<01:02,  1.10s/it, train_loss=29.6]\u001b[A\n",
            " 51% 59/115 [00:56<01:01,  1.10s/it, train_loss=29.6]\u001b[A\n",
            " 51% 59/115 [00:57<01:01,  1.10s/it, train_loss=29.2]\u001b[A\n",
            " 52% 60/115 [00:57<00:58,  1.07s/it, train_loss=29.2]\u001b[A\n",
            " 52% 60/115 [00:58<00:58,  1.07s/it, train_loss=28.8]\u001b[A\n",
            " 53% 61/115 [00:58<00:58,  1.08s/it, train_loss=28.8]\u001b[A\n",
            " 53% 61/115 [00:59<00:58,  1.08s/it, train_loss=28.4]\u001b[A\n",
            " 54% 62/115 [00:59<00:58,  1.10s/it, train_loss=28.4]\u001b[A\n",
            " 54% 62/115 [01:01<00:58,  1.10s/it, train_loss=28.1]\u001b[A\n",
            " 55% 63/115 [01:01<01:01,  1.18s/it, train_loss=28.1]\u001b[A\n",
            " 55% 63/115 [01:02<01:01,  1.18s/it, train_loss=27.7]\u001b[A\n",
            " 56% 64/115 [01:02<00:59,  1.17s/it, train_loss=27.7]\u001b[A\n",
            " 56% 64/115 [01:03<00:59,  1.17s/it, train_loss=27.4]\u001b[A\n",
            " 57% 65/115 [01:03<00:58,  1.17s/it, train_loss=27.4]\u001b[A\n",
            " 57% 65/115 [01:04<00:58,  1.17s/it, train_loss=27]  \u001b[A\n",
            " 57% 66/115 [01:04<00:55,  1.14s/it, train_loss=27]\u001b[A\n",
            " 57% 66/115 [01:05<00:55,  1.14s/it, train_loss=26.7]\u001b[A\n",
            " 58% 67/115 [01:05<00:52,  1.10s/it, train_loss=26.7]\u001b[A\n",
            " 58% 67/115 [01:06<00:52,  1.10s/it, train_loss=26.4]\u001b[A\n",
            " 59% 68/115 [01:06<00:53,  1.14s/it, train_loss=26.4]\u001b[A\n",
            " 59% 68/115 [01:07<00:53,  1.14s/it, train_loss=26.1]\u001b[A\n",
            " 60% 69/115 [01:07<00:51,  1.13s/it, train_loss=26.1]\u001b[A\n",
            " 60% 69/115 [01:09<00:51,  1.13s/it, train_loss=25.8]\u001b[A\n",
            " 61% 70/115 [01:09<00:52,  1.17s/it, train_loss=25.8]\u001b[A\n",
            " 61% 70/115 [01:10<00:52,  1.17s/it, train_loss=25.6]\u001b[A\n",
            " 62% 71/115 [01:10<00:51,  1.16s/it, train_loss=25.6]\u001b[A\n",
            " 62% 71/115 [01:11<00:51,  1.16s/it, train_loss=25.3]\u001b[A\n",
            " 63% 72/115 [01:11<00:51,  1.19s/it, train_loss=25.3]\u001b[A\n",
            " 63% 72/115 [01:12<00:51,  1.19s/it, train_loss=25]  \u001b[A\n",
            " 63% 73/115 [01:12<00:50,  1.20s/it, train_loss=25]\u001b[A\n",
            " 63% 73/115 [01:14<00:50,  1.20s/it, train_loss=24.7]\u001b[A\n",
            " 64% 74/115 [01:14<00:50,  1.24s/it, train_loss=24.7]\u001b[A\n",
            " 64% 74/115 [01:15<00:50,  1.24s/it, train_loss=24.5]\u001b[A\n",
            " 65% 75/115 [01:15<00:48,  1.21s/it, train_loss=24.5]\u001b[A\n",
            " 65% 75/115 [01:16<00:48,  1.21s/it, train_loss=24.2]\u001b[A\n",
            " 66% 76/115 [01:16<00:46,  1.20s/it, train_loss=24.2]\u001b[A\n",
            " 66% 76/115 [01:17<00:46,  1.20s/it, train_loss=24]  \u001b[A\n",
            " 67% 77/115 [01:17<00:45,  1.19s/it, train_loss=24]\u001b[A\n",
            " 67% 77/115 [01:18<00:45,  1.19s/it, train_loss=23.8]\u001b[A\n",
            " 68% 78/115 [01:18<00:45,  1.24s/it, train_loss=23.8]\u001b[A\n",
            " 68% 78/115 [01:20<00:45,  1.24s/it, train_loss=23.5]\u001b[A\n",
            " 69% 79/115 [01:20<00:45,  1.28s/it, train_loss=23.5]\u001b[A\n",
            " 69% 79/115 [01:21<00:45,  1.28s/it, train_loss=23.3]\u001b[A\n",
            " 70% 80/115 [01:21<00:45,  1.30s/it, train_loss=23.3]\u001b[A\n",
            " 70% 80/115 [01:23<00:45,  1.30s/it, train_loss=23.1]\u001b[A\n",
            " 70% 81/115 [01:23<00:45,  1.34s/it, train_loss=23.1]\u001b[A\n",
            " 70% 81/115 [01:24<00:45,  1.34s/it, train_loss=22.9]\u001b[A\n",
            " 71% 82/115 [01:24<00:43,  1.32s/it, train_loss=22.9]\u001b[A\n",
            " 71% 82/115 [01:25<00:43,  1.32s/it, train_loss=22.7]\u001b[A\n",
            " 72% 83/115 [01:25<00:42,  1.33s/it, train_loss=22.7]\u001b[A\n",
            " 72% 83/115 [01:26<00:42,  1.33s/it, train_loss=22.5]\u001b[A\n",
            " 73% 84/115 [01:26<00:40,  1.32s/it, train_loss=22.5]\u001b[A\n",
            " 73% 84/115 [01:28<00:40,  1.32s/it, train_loss=22.3]\u001b[A\n",
            " 74% 85/115 [01:28<00:39,  1.32s/it, train_loss=22.3]\u001b[A\n",
            " 74% 85/115 [01:29<00:39,  1.32s/it, train_loss=22.1]\u001b[A\n",
            " 75% 86/115 [01:29<00:38,  1.32s/it, train_loss=22.1]\u001b[A\n",
            " 75% 86/115 [01:30<00:38,  1.32s/it, train_loss=21.9]\u001b[A\n",
            " 76% 87/115 [01:30<00:37,  1.33s/it, train_loss=21.9]\u001b[A\n",
            " 76% 87/115 [01:32<00:37,  1.33s/it, train_loss=21.7]\u001b[A\n",
            " 77% 88/115 [01:32<00:35,  1.32s/it, train_loss=21.7]\u001b[A\n",
            " 77% 88/115 [01:33<00:35,  1.32s/it, train_loss=21.5]\u001b[A\n",
            " 77% 89/115 [01:33<00:34,  1.34s/it, train_loss=21.5]\u001b[A\n",
            " 77% 89/115 [01:35<00:34,  1.34s/it, train_loss=21.4]\u001b[A\n",
            " 78% 90/115 [01:35<00:33,  1.34s/it, train_loss=21.4]\u001b[A\n",
            " 78% 90/115 [01:36<00:33,  1.34s/it, train_loss=21.2]\u001b[A\n",
            " 79% 91/115 [01:36<00:32,  1.36s/it, train_loss=21.2]\u001b[A\n",
            " 79% 91/115 [01:37<00:32,  1.36s/it, train_loss=21]  \u001b[A\n",
            " 80% 92/115 [01:37<00:31,  1.35s/it, train_loss=21]\u001b[A\n",
            " 80% 92/115 [01:39<00:31,  1.35s/it, train_loss=20.8]\u001b[A\n",
            " 81% 93/115 [01:39<00:29,  1.34s/it, train_loss=20.8]\u001b[A\n",
            " 81% 93/115 [01:40<00:29,  1.34s/it, train_loss=20.7]\u001b[A\n",
            " 82% 94/115 [01:40<00:28,  1.35s/it, train_loss=20.7]\u001b[A\n",
            " 82% 94/115 [01:41<00:28,  1.35s/it, train_loss=20.5]\u001b[A\n",
            " 83% 95/115 [01:41<00:27,  1.35s/it, train_loss=20.5]\u001b[A\n",
            " 83% 95/115 [01:43<00:27,  1.35s/it, train_loss=20.4]\u001b[A\n",
            " 83% 96/115 [01:43<00:27,  1.42s/it, train_loss=20.4]\u001b[A\n",
            " 83% 96/115 [01:44<00:27,  1.42s/it, train_loss=20.2]\u001b[A\n",
            " 84% 97/115 [01:44<00:25,  1.41s/it, train_loss=20.2]\u001b[A\n",
            " 84% 97/115 [01:46<00:25,  1.41s/it, train_loss=20.1]\u001b[A\n",
            " 85% 98/115 [01:46<00:23,  1.36s/it, train_loss=20.1]\u001b[A\n",
            " 85% 98/115 [01:47<00:23,  1.36s/it, train_loss=19.9]\u001b[A\n",
            " 86% 99/115 [01:47<00:23,  1.47s/it, train_loss=19.9]\u001b[A\n",
            " 86% 99/115 [01:49<00:23,  1.47s/it, train_loss=19.8]\u001b[A\n",
            " 87% 100/115 [01:49<00:22,  1.52s/it, train_loss=19.8]\u001b[A\n",
            " 87% 100/115 [01:50<00:22,  1.52s/it, train_loss=19.6]\u001b[A\n",
            " 88% 101/115 [01:50<00:20,  1.48s/it, train_loss=19.6]\u001b[A\n",
            " 88% 101/115 [01:52<00:20,  1.48s/it, train_loss=19.5]\u001b[A\n",
            " 89% 102/115 [01:52<00:20,  1.55s/it, train_loss=19.5]\u001b[A\n",
            " 89% 102/115 [01:53<00:20,  1.55s/it, train_loss=19.4]\u001b[A\n",
            " 90% 103/115 [01:53<00:18,  1.52s/it, train_loss=19.4]\u001b[A\n",
            " 90% 103/115 [01:55<00:18,  1.52s/it, train_loss=19.2]\u001b[A\n",
            " 90% 104/115 [01:55<00:17,  1.58s/it, train_loss=19.2]\u001b[A\n",
            " 90% 104/115 [01:57<00:17,  1.58s/it, train_loss=19.1]\u001b[A\n",
            " 91% 105/115 [01:57<00:16,  1.61s/it, train_loss=19.1]\u001b[A\n",
            " 91% 105/115 [01:59<00:16,  1.61s/it, train_loss=19]  \u001b[A\n",
            " 92% 106/115 [01:59<00:14,  1.66s/it, train_loss=19]\u001b[A\n",
            " 92% 106/115 [02:00<00:14,  1.66s/it, train_loss=18.9]\u001b[A\n",
            " 93% 107/115 [02:00<00:13,  1.65s/it, train_loss=18.9]\u001b[A\n",
            " 93% 107/115 [02:02<00:13,  1.65s/it, train_loss=18.7]\u001b[A\n",
            " 94% 108/115 [02:02<00:11,  1.64s/it, train_loss=18.7]\u001b[A\n",
            " 94% 108/115 [02:04<00:11,  1.64s/it, train_loss=18.6]\u001b[A\n",
            " 95% 109/115 [02:04<00:10,  1.67s/it, train_loss=18.6]\u001b[A\n",
            " 95% 109/115 [02:05<00:10,  1.67s/it, train_loss=18.5]\u001b[A\n",
            " 96% 110/115 [02:05<00:08,  1.70s/it, train_loss=18.5]\u001b[A\n",
            " 96% 110/115 [02:07<00:08,  1.70s/it, train_loss=18.4]\u001b[A\n",
            " 97% 111/115 [02:07<00:06,  1.75s/it, train_loss=18.4]\u001b[A\n",
            " 97% 111/115 [02:10<00:06,  1.75s/it, train_loss=18.3]\u001b[A\n",
            " 97% 112/115 [02:10<00:05,  1.91s/it, train_loss=18.3]\u001b[A\n",
            " 97% 112/115 [02:12<00:05,  1.91s/it, train_loss=18.1]\u001b[A\n",
            " 98% 113/115 [02:12<00:04,  2.03s/it, train_loss=18.1]\u001b[A\n",
            " 98% 113/115 [02:14<00:04,  2.03s/it, train_loss=18]  \u001b[A\n",
            " 99% 114/115 [02:14<00:02,  2.13s/it, train_loss=18]\u001b[A\n",
            " 99% 114/115 [02:17<00:02,  2.13s/it, train_loss=17.9]\u001b[A\n",
            "100% 115/115 [02:17<00:00,  1.20s/it, train_loss=17.9]\n",
            "100% 260/260 [00:37<00:00,  6.89it/s]\n",
            "speechbrain.utils.train_logger - epoch: 1, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 17.93 - valid loss: 5.44, valid CER: 99.79, valid WER: 1.00e+02\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+21-45-01+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 2\n",
            "100% 115/115 [02:25<00:00,  1.26s/it, train_loss=5.01]\n",
            "100% 260/260 [00:37<00:00,  6.93it/s]\n",
            "speechbrain.utils.train_logger - epoch: 2, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 5.01 - valid loss: 5.13, valid CER: 1.00e+02, valid WER: 99.97\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+21-48-53+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+21-45-01+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 3\n",
            "100% 115/115 [02:23<00:00,  1.25s/it, train_loss=4.52]\n",
            "100% 260/260 [00:37<00:00,  6.95it/s]\n",
            "speechbrain.utils.train_logger - epoch: 3, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 4.52 - valid loss: 4.12, valid CER: 99.87, valid WER: 99.87\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+21-52-53+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+21-48-53+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 4\n",
            "100% 115/115 [02:25<00:00,  1.26s/it, train_loss=3.87]\n",
            "100% 260/260 [00:37<00:00,  6.90it/s]\n",
            "speechbrain.utils.train_logger - epoch: 4, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 3.87 - valid loss: 3.57, valid CER: 87.78, valid WER: 91.74\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+21-57-17+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+21-52-53+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 5\n",
            "100% 115/115 [02:25<00:00,  1.26s/it, train_loss=3.19]\n",
            "100% 260/260 [00:37<00:00,  6.96it/s]\n",
            "speechbrain.utils.train_logger - epoch: 5, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 3.19 - valid loss: 2.66, valid CER: 69.05, valid WER: 85.84\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-01-10+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+21-57-17+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 6\n",
            "100% 115/115 [02:26<00:00,  1.27s/it, train_loss=2.61]\n",
            "100% 260/260 [00:37<00:00,  6.93it/s]\n",
            "speechbrain.utils.train_logger - epoch: 6, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 2.61 - valid loss: 2.33, valid CER: 53.84, valid WER: 78.82\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-04-33+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-01-10+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 7\n",
            "100% 115/115 [02:24<00:00,  1.25s/it, train_loss=2.18]\n",
            "100% 260/260 [00:38<00:00,  6.76it/s]\n",
            "speechbrain.utils.train_logger - epoch: 7, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 2.18 - valid loss: 2.16, valid CER: 51.63, valid WER: 75.96\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-08-05+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-04-33+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 8\n",
            "100% 115/115 [02:25<00:00,  1.26s/it, train_loss=1.86]\n",
            "100% 260/260 [00:38<00:00,  6.83it/s]\n",
            "speechbrain.utils.train_logger - epoch: 8, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 1.86 - valid loss: 1.87, valid CER: 39.36, valid WER: 70.53\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-11-34+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-08-05+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 9\n",
            "100% 115/115 [02:25<00:00,  1.26s/it, train_loss=1.68]\n",
            "100% 260/260 [00:37<00:00,  6.91it/s]\n",
            "speechbrain.utils.train_logger - epoch: 9, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 1.68 - valid loss: 1.72, valid CER: 35.02, valid WER: 66.69\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-15-01+00\n",
            "speechbrain.utils.checkpoints - Deleted checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-11-34+00\n",
            "speechbrain.utils.epoch_loop - Going into epoch 10\n",
            "100% 115/115 [02:24<00:00,  1.26s/it, train_loss=1.46]\n",
            "100% 260/260 [00:37<00:00,  6.93it/s]\n",
            "speechbrain.nnet.schedulers - Changing lr from 1 to 0.8\n",
            "speechbrain.nnet.schedulers - Changing lr from 0.0001 to 9e-05\n",
            "speechbrain.utils.train_logger - epoch: 10, lr_model: 1.00e+00, lr_wav2vec: 1.00e-04 - train loss: 1.46 - valid loss: 1.76, valid CER: 36.45, valid WER: 66.90\n",
            "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-18-37+00\n",
            "speechbrain.utils.checkpoints - Loading a checkpoint from results/wav2vec2_ctc_rw/1986/save/CKPT+2025-03-26+22-15-01+00\n",
            "100% 278/278 [00:57<00:00,  4.82it/s]\n",
            "speechbrain.utils.train_logger - Epoch loaded: 9 - test loss: 1.70, test CER: 32.41, test WER: 65.05\n"
          ]
        }
      ],
      "source": [
        "# Remember: Delete the output folder to start training from scratch\n",
        "!rm -rf ./results/*\n",
        "\n",
        "# Run Training\n",
        "!python train_with_wav2vec.py hparams_sr_wav2vec.yaml  --data_folder='/content/' --device='cuda:0' --number_of_epochs=10 --seed=1986"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Han8STfzstpw"
      },
      "source": [
        "The expected error rate in testing is around 80%. But don't lose hope! Adding more data could make the model much better. We suggest fine-tuning the model with [Whisper](https://arxiv.org/abs/2212.04356) for extra practice.\n",
        "You can find examples on how to do this [here](https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice/ASR/transformer)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00b0f6eeef7f4d7e927d09e59235a5d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03593be6841e40b184dbea8c1f0ddec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ae836af6d5ce4b9684be36a77ab10d48"
          }
        },
        "0834dab8cd8841c2b0f28c39088ab774": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_250c7868d83a46c188279dfc57b0a082",
            "style": "IPY_MODEL_e3f0b8ca76d940e5804b88f12d4acfaa",
            "tooltip": ""
          }
        },
        "0baf9d9792ae4fe29d853c331bf90e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "250c7868d83a46c188279dfc57b0a082": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3039120518c84a3487dc314c9bafa582": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "303a1bbb8296463bbf0b025c54d80b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b90aecf8c104eef866ba6fbab336462": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70af752d2ee648cea74f29915a2814fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0e7ef77f95f49a4a068da8edfe48d6c",
            "placeholder": "​",
            "style": "IPY_MODEL_5b90aecf8c104eef866ba6fbab336462",
            "value": "Connecting..."
          }
        },
        "733bbad1adcb43e883be259c583627b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8e9e5dc25fd44728afd446f334706710",
            "placeholder": "​",
            "style": "IPY_MODEL_303a1bbb8296463bbf0b025c54d80b77",
            "value": ""
          }
        },
        "77016ab67dec42079a1a34974fb62339": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0baf9d9792ae4fe29d853c331bf90e4f",
            "placeholder": "​",
            "style": "IPY_MODEL_e4d6fec0010643eb8bea9f4921934b8f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "7bfeee9849bd4466a63a847afd390ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00b0f6eeef7f4d7e927d09e59235a5d9",
            "placeholder": "​",
            "style": "IPY_MODEL_9dedb09af1684e3ba9ffbbfcbea2dc2b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "8e9e5dc25fd44728afd446f334706710": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91d19be416a74c36a7d87ebd1f8a0b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dedb09af1684e3ba9ffbbfcbea2dc2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae836af6d5ce4b9684be36a77ab10d48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e0e7ef77f95f49a4a068da8edfe48d6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f0b8ca76d940e5804b88f12d4acfaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e4d6fec0010643eb8bea9f4921934b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0fd2e9465ef4a4ca0aa75952fd29870": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_3039120518c84a3487dc314c9bafa582",
            "style": "IPY_MODEL_91d19be416a74c36a7d87ebd1f8a0b4a",
            "value": true
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
